{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Processing Hadith Dataset\n",
    "\n",
    "1. **Define Validation Models and Utilities**\n",
    "   - Define a Pydantic model (`QuestionAnswer`) for validating question-answer pairs.\n",
    "   - Implement a function (`estimate_tokens`) to estimate the number of tokens in Arabic text.\n",
    "\n",
    "2. **Prepare the Prompt Template**\n",
    "   - Create a prompt template (`PROMPT_TEMPLATE`) that instructs the language model to answer four fixed questions about each hadith, using Modern Standard Arabic with full diacritics.\n",
    "\n",
    "3. **Process a Single Hadith Dataset File**\n",
    "   - Load the input JSON file containing hadith entries.\n",
    "   - For each entry:\n",
    "     - Skip entries without a valid explanation (`sharh`).\n",
    "     - Format the prompt with the hadith and its explanation.\n",
    "     - Estimate the number of tokens in the prompt.\n",
    "     - Send the prompt to the Together API and stream the response.\n",
    "     - Collect and clean the model's output, ensuring it is a JSON array of four answers.\n",
    "     - Map each answer to its corresponding question and update the entry with:\n",
    "       - `FT_Pairs`: List of question-answer pairs.\n",
    "       - `hadith_lessons`: First answer as a lesson.\n",
    "       - `hadith_application`: Second answer as an application.\n",
    "     - Handle errors by setting empty values if processing fails.\n",
    "   - Save the enriched data to the output JSON file.\n",
    "   - Print statistics about the processing.\n",
    "\n",
    "4. **Process All Hadith Files in a Directory**\n",
    "   - Create the output directory if it does not exist.\n",
    "   - Find all JSON files in the input directory.\n",
    "   - For each file:\n",
    "     - Process the file using the above function.\n",
    "     - Aggregate statistics across all files.\n",
    "     - Add a delay between files to avoid rate limiting.\n",
    "   - Print an overall summary of the processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Together' from 'together' (/home/mohamed/.local/lib/python3.12/site-packages/together/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Together' from 'together' (/home/mohamed/.local/lib/python3.12/site-packages/together/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from together import Together\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Together client\n",
    "client = Together(api_key=\"tgp_v1_nG8tM-osJ_jwKSPsPiRZnr6IiartvAs5A8IexEAoyxk\")\n",
    "\n",
    "# Define fixed questions with diacritics\n",
    "def get_fixed_questions() -> List[Dict[str, str]]:\n",
    "    \"\"\"Return the fixed questions with their full diacritics\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"question\": \"Ù…ÙØ§ Ù‡ÙÙŠÙ Ø§Ù„Ø±ÙÙ‘Ø³ÙØ§Ø¦ÙÙ„Ù Ø§Ù„Ø±ÙÙ‘Ø¦ÙÙŠØ³ÙÙŠÙÙ‘Ø©Ù ÙˆÙØ§Ù„Ø¯ÙÙ‘Ø±ÙÙˆØ³Ù Ø§Ù„Ù…ÙØ³Ù’ØªÙÙÙØ§Ø¯ÙØ©Ù ÙˆØ§Ù„ÙÙÙˆÙØ§Ø¦ÙØ¯ Ø§Ù„Ù…ÙØ³ØªÙØ®Ù„ØµÙØ© Ù…ÙÙ†Ù Ø§Ù„Ø­ÙØ¯ÙÙŠØ«ÙØŸ\",\n",
    "            \"answer\": \"{answer1}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ÙƒÙÙŠÙ’ÙÙ ÙŠÙÙ…Ù’ÙƒÙÙ†Ù ØªÙØ·Ù’Ø¨ÙÙŠÙ‚Ù Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù ÙÙÙŠ Ø§Ù„Ø­ÙÙŠÙØ§Ø©Ù Ø§Ù„ÙŠÙÙˆÙ’Ù…ÙÙŠÙÙ‘Ø©ÙØŸ\",\n",
    "            \"answer\": \"{answer2}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Ù…ÙØ§ Ø£ÙÙ‡ÙÙ…ÙÙ‘ÙŠÙÙ‘Ø©Ù Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù ÙÙÙŠ Ø§Ù„ÙÙÙ‚Ù’Ù‡Ù Ø§Ù„Ø¥ÙØ³Ù’Ù„ÙØ§Ù…ÙÙŠÙÙ‘ØŸ\",\n",
    "            \"answer\": \"{answer3}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Ù…ÙØ§ Ø³ÙØ¨ÙØ¨Ù ÙˆÙØ±ÙÙˆØ¯Ù Ø§Ù„Ø­ÙØ¯ÙÙŠØ«ÙØŸ\",\n",
    "            \"answer\": \"{answer4}\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for validation\n",
    "class QuestionAnswer(BaseModel):\n",
    "    \"\"\"Model for question-answer pairs\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# Function to estimate token count (rough approximation)\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count in a string (rough approximation)\"\"\"\n",
    "    # For Arabic text, a rough estimation is about 1 token per 2.5 characters\n",
    "    return len(text) // 2\n",
    "\n",
    "# Create a simplified prompt template that only asks for answers, not questions\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in analyzing Prophetic Hadiths and Islamic jurisprudence.\n",
    "Your task is to analyze an Arabic Hadith using the provided explanation to extract knowledge, jurisprudential rulings, and practical applications.\n",
    "\n",
    "== INPUT ==\n",
    "You will receive:\n",
    "- 'hadith': Text of the Prophetic Hadith in Arabic (with diacritics).\n",
    "- 'explanation': Detailed explanation of the Hadith (Arabic text).\n",
    "\n",
    "== TASK ==\n",
    "Based on the provided Hadith, explanation, and your knowledge, answer the following questions in JSON format.\n",
    "\n",
    "== FIXED QUESTIONS ==\n",
    "Answer ONLY these questions without repeating the question text:\n",
    "1. What are the main messages, lessons learned, and benefits derived from the Hadith?\n",
    "2. How can the Hadith be applied in daily life?\n",
    "3. What is the importance of the Hadith in Islamic jurisprudence?\n",
    "4. What is the reason or context behind the narration of the Hadith?\n",
    "\n",
    "== RULES ==\n",
    "- Use Modern Standard Arabic with full diacritics in your answers.\n",
    "- Base your answers on the provided explanation and your knowledge of authentic Hadiths.\n",
    "- Ensure each answer accurately reflects the content and meaning of the Hadith without incorporating personal interpretations or conclusions.\n",
    "- Verify the authenticity of all information before preparing your response.\n",
    "- Return ONLY a JSON array with your answers as shown below, without any additional comments or explanations.\n",
    "\n",
    "== Expected Input ==\n",
    "{{\n",
    "  \"hadith\": \"{hadith}\",\n",
    "  \"sharh\": \"{sharh}\"\n",
    "}}\n",
    "\n",
    "== Expected Output (JSON Array) ==\n",
    "[\n",
    "  \"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„...\",\n",
    "  \"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„...\",\n",
    "  \"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„...\",\n",
    "  \"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ù…Ø¹ Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„...\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def process_hadith_dataset(input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Process the hadith dataset, enriching it with QA pairs\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input JSON file\n",
    "        output_file: Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    # Load hadith dataset\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get fixed questions template\n",
    "    fixed_questions = get_fixed_questions()\n",
    "    \n",
    "    # Process each hadith with a progress bar\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(f\"Processing {len(data)} hadith entries from {input_file}...\")\n",
    "    for entry in tqdm(data, desc=f\"Processing hadiths from {os.path.basename(input_file)}\"):\n",
    "        # Skip if no sharh is available\n",
    "        if not entry.get(\"sharh\") or entry[\"sharh\"] == \".\":\n",
    "            print(f\"Skipping hadith ID {entry.get('hadith_id', 'unknown')} - no sharh available\")\n",
    "            continue\n",
    "        \n",
    "        # Create input for LLM - use the same file without creating a new one\n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            hadith=entry[\"hadith\"],\n",
    "            sharh=entry[\"sharh\"]\n",
    "        )\n",
    "        \n",
    "        # Estimate tokens before generation\n",
    "        tokens_before = estimate_tokens(prompt)\n",
    "        total_tokens_before += tokens_before\n",
    "        \n",
    "        # Stream response from Together\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-V3\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Collect response\n",
    "            output_text = \"\"\n",
    "            for token in response:\n",
    "                if hasattr(token, 'choices') and token.choices[0].delta.content:\n",
    "                    output_text += token.choices[0].delta.content\n",
    "            \n",
    "            # Estimate tokens after generation\n",
    "            tokens_after = estimate_tokens(output_text)\n",
    "            total_tokens_after += tokens_after\n",
    "            \n",
    "            # Process the output - directly match keys and values\n",
    "            try:\n",
    "                # Clean the output text - handle potential formatting issues\n",
    "                output_text = output_text.strip()\n",
    "                if output_text.startswith(\"```json\"):\n",
    "                    output_text = output_text[7:]\n",
    "                if output_text.endswith(\"```\"):\n",
    "                    output_text = output_text[:-3]\n",
    "                output_text = output_text.strip()\n",
    "                \n",
    "                # Parse as regular JSON - expecting an array of 4 strings\n",
    "                answer_list = json.loads(output_text)\n",
    "                \n",
    "                if not isinstance(answer_list, list):\n",
    "                    raise ValueError(f\"Expected list of answers, got: {type(answer_list)}\")\n",
    "                \n",
    "                # Ensure we have at least 4 answers, pad with empty strings if needed\n",
    "                while len(answer_list) < 4:\n",
    "                    answer_list.append(\"\")\n",
    "                \n",
    "                # Use direct key matching instead of creating new objects\n",
    "                qa_pairs = []\n",
    "                for i, answer in enumerate(answer_list[:4]):  # Limit to first 4 answers\n",
    "                    qa_pair = {\n",
    "                        \"question\": fixed_questions[i][\"question\"],\n",
    "                        \"answer\": answer\n",
    "                    }\n",
    "                    qa_pairs.append(qa_pair)\n",
    "                \n",
    "                # Update the existing entry directly\n",
    "                entry[\"FT_Pairs\"] = qa_pairs\n",
    "                \n",
    "                # Extract first two answers directly into lessons and application\n",
    "                entry[\"hadith_lessons\"] = [answer_list[0]] if answer_list[0] else []\n",
    "                entry[\"hadith_application\"] = [answer_list[1]] if answer_list[1] else []\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"\\n[!] Processing error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "                # Set empty values for failed processing\n",
    "                entry[\"FT_Pairs\"] = []\n",
    "                entry[\"hadith_lessons\"] = []\n",
    "                entry[\"hadith_application\"] = []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] API error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "            entry[\"FT_Pairs\"] = []\n",
    "            entry[\"hadith_lessons\"] = []\n",
    "            entry[\"hadith_application\"] = []\n",
    "    \n",
    "    # Save the enriched data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nProcessing completed for file {input_file}:\")\n",
    "    print(f\"- Successfully processed: {processed_count}/{len(data)} entries\")\n",
    "    print(f\"- Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"- Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"- Token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"- Dataset saved to '{output_file}'\")\n",
    "    \n",
    "    return processed_count, total_tokens_before, total_tokens_after\n",
    "\n",
    "def process_all_hadith_files(input_dir: str, output_dir: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Process all hadith JSON files in a directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to the directory containing input JSON files\n",
    "        output_dir: Path to save the output JSON files (if None, will use input_dir with '_processed' suffix)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if output_dir is None:\n",
    "        output_dir = input_dir + \"_processed\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files in the input directory\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    total_processed = 0\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    \n",
    "    for input_file in json_files:\n",
    "        file_name = os.path.basename(input_file)\n",
    "        output_file = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        print(f\"\\nProcessing file: {file_name}\")\n",
    "        processed, tokens_before, tokens_after = process_hadith_dataset(input_file, output_file)\n",
    "        \n",
    "        total_processed += processed\n",
    "        total_tokens_before += tokens_before\n",
    "        total_tokens_after += tokens_after\n",
    "        \n",
    "        # Add a small delay between files to avoid rate limiting\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"\\n===== OVERALL PROCESSING SUMMARY =====\")\n",
    "    print(f\"Total files processed: {len(json_files)}\")\n",
    "    print(f\"Total hadiths processed: {total_processed}\")\n",
    "    print(f\"Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"Overall token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"All processed files saved to '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files in the Sahih_muslim directory\n",
    "    input_dir = r\"d:\\Ø²ÙŠÙ†Ø¨\\Sahih_muslim\"\n",
    "    output_dir = r\"d:\\Ø²ÙŠÙ†Ø¨\\Sahih_muslim_processed\"\n",
    "    \n",
    "    process_all_hadith_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the mattan's hadith to questions answers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # Ù…ÙƒØªØ¨Ø© Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…\n",
    "\n",
    "# Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù„ÙØ§Øª JSON\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ tqdm Ù„Ø¹Ø±Ø¶ Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…\n",
    "for filename in tqdm(json_files, desc=\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª\"):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # ÙØªØ­ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø¹Ù†ØµØ± ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    for entry in data:\n",
    "        hadith_text = entry.get(\"hadith\", \"\")\n",
    "\n",
    "        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙÙŠ FT_Pairs\n",
    "        for pair in entry.get(\"FT_Pairs\", []):\n",
    "            if \"question\" in pair:\n",
    "                pair[\"question\"] = pair[\"question\"].replace(\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù\", f\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù'{hadith_text}'\")\n",
    "\n",
    "        # ØªØ­Ø¯ÙŠØ« hadith_lessons Ùˆ hadith_application Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©\n",
    "        if \"hadith_lessons\" in entry and entry[\"hadith_lessons\"]:\n",
    "            for i, lesson in enumerate(entry[\"hadith_lessons\"]):\n",
    "                entry[\"hadith_lessons\"][i] = lesson.replace(\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù\", f\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù'{hadith_text}'\")\n",
    "        \n",
    "        if \"hadith_application\" in entry and entry[\"hadith_application\"]:\n",
    "            for i, app in enumerate(entry[\"hadith_application\"]):\n",
    "                entry[\"hadith_application\"][i] = app.replace(\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù\", f\"Ø§Ù„Ø­ÙØ¯ÙÙŠØ«Ù'{hadith_text}'\")\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ« Ø¨Ù†Ø¬Ø§Ø­!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Empty JSON Files to Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Ù…Ø¬Ù„Ø¯ Ù…Ù„ÙØ§Øª JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª JSON ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # ÙØªØ­ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ±\n",
    "            if isinstance(data, list):\n",
    "                all_empty = True\n",
    "                for entry in data:\n",
    "                    # ÙØ­Øµ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø«Ù„Ø§Ø«Ø©\n",
    "                    if (entry.get(\"hadith_lessons\") or \n",
    "                        entry.get(\"hadith_application\") or \n",
    "                        entry.get(\"FT_Pairs\")):\n",
    "                        all_empty = False\n",
    "                        break\n",
    "\n",
    "                # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ø­Ù‚ÙˆÙ„ ÙÙŠ ÙƒÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± ÙØ§Ø±ØºØ©\n",
    "                if all_empty:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ù„Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ ÙØ§Ø±ØºØ©: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Processed JSON Files from Original Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ù…Ø¬Ù„Ø¯ Ù…Ù„ÙØ§Øª JSON Ø§Ù„Ø£ØµÙ„ÙŠØ©\n",
    "processed_folder = r\"D:\\Ø²ÙŠÙ†Ø¨\\Sahih_muslim_processed\"\n",
    "\n",
    "# Ù…Ø¬Ù„Ø¯ ØµØ­ÙŠØ­ Ù…Ø³Ù„Ù… Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù†Ù‡\n",
    "original_folder = r\"D:\\Ø²ÙŠÙ†Ø¨\\Sahih_muslim\"\n",
    "\n",
    "# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ù…Ù„ÙØ§Øª JSON Ù…Ù† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯)\n",
    "json_names = [os.path.splitext(f)[0] for f in os.listdir(processed_folder) if f.endswith(\".json\")]\n",
    "\n",
    "# Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ ØµØ­ÙŠØ­ Ù…Ø³Ù„Ù…\n",
    "for filename in os.listdir(original_folder):\n",
    "    file_path = os.path.join(original_folder, filename)\n",
    "\n",
    "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ù† ÙƒØ§Ù† Ù‡Ø°Ø§ Ù…Ù„Ù JSON ÙˆØ§Ø³Ù…Ù‡ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡\n",
    "    if filename.endswith(\".json\"):\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        if name_without_ext in json_names:\n",
    "            os.remove(file_path)\n",
    "            print(f\"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù: {filename}\")\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù…Ø¬Ù„Ø¯ Sahih_muslim.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
