{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Processing Hadith Dataset\n",
    "\n",
    "1. **Define Validation Models and Utilities**\n",
    "   - Define a Pydantic model (`QuestionAnswer`) for validating question-answer pairs.\n",
    "   - Implement a function (`estimate_tokens`) to estimate the number of tokens in Arabic text.\n",
    "\n",
    "2. **Prepare the Prompt Template**\n",
    "   - Create a prompt template (`PROMPT_TEMPLATE`) that instructs the language model to answer four fixed questions about each hadith, using Modern Standard Arabic with full diacritics.\n",
    "\n",
    "3. **Process a Single Hadith Dataset File**\n",
    "   - Load the input JSON file containing hadith entries.\n",
    "   - For each entry:\n",
    "     - Skip entries without a valid explanation (`sharh`).\n",
    "     - Format the prompt with the hadith and its explanation.\n",
    "     - Estimate the number of tokens in the prompt.\n",
    "     - Send the prompt to the Together API and stream the response.\n",
    "     - Collect and clean the model's output, ensuring it is a JSON array of four answers.\n",
    "     - Map each answer to its corresponding question and update the entry with:\n",
    "       - `FT_Pairs`: List of question-answer pairs.\n",
    "       - `hadith_lessons`: First answer as a lesson.\n",
    "       - `hadith_application`: Second answer as an application.\n",
    "     - Handle errors by setting empty values if processing fails.\n",
    "   - Save the enriched data to the output JSON file.\n",
    "   - Print statistics about the processing.\n",
    "\n",
    "4. **Process All Hadith Files in a Directory**\n",
    "   - Create the output directory if it does not exist.\n",
    "   - Find all JSON files in the input directory.\n",
    "   - For each file:\n",
    "     - Process the file using the above function.\n",
    "     - Aggregate statistics across all files.\n",
    "     - Add a delay between files to avoid rate limiting.\n",
    "   - Print an overall summary of the processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from together import Together\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Together client\n",
    "client = Together(api_key=\"tgp_v1_nG8tM-osJ_jwKSPsPiRZnr6IiartvAs5A8IexEAoyxk\")\n",
    "\n",
    "# Define fixed questions with diacritics\n",
    "def get_fixed_questions() -> List[Dict[str, str]]:\n",
    "    \"\"\"Return the fixed questions with their full diacritics\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"question\": \"مَا هِيَ الرَّسَائِلُ الرَّئِيسِيَّةُ وَالدُّرُوسُ المُسْتَفَادَةُ والفَوَائِد المُستَخلصَة مِنَ الحَدِيثِ؟\",\n",
    "            \"answer\": \"{answer1}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"كَيْفَ يُمْكِنُ تَطْبِيقُ الحَدِيثِ فِي الحَيَاةِ اليَوْمِيَّةِ؟\",\n",
    "            \"answer\": \"{answer2}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"مَا أَهَمِّيَّةُ الحَدِيثِ فِي الفِقْهِ الإِسْلَامِيِّ؟\",\n",
    "            \"answer\": \"{answer3}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"مَا سَبَبُ وُرُودِ الحَدِيثِ؟\",\n",
    "            \"answer\": \"{answer4}\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for validation\n",
    "class QuestionAnswer(BaseModel):\n",
    "    \"\"\"Model for question-answer pairs\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# Function to estimate token count (rough approximation)\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count in a string (rough approximation)\"\"\"\n",
    "    # For Arabic text, a rough estimation is about 1 token per 2.5 characters\n",
    "    return len(text) // 2\n",
    "\n",
    "# Create a simplified prompt template that only asks for answers, not questions\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in analyzing Prophetic Hadiths and Islamic jurisprudence.\n",
    "Your task is to analyze an Arabic Hadith using the provided explanation to extract knowledge, jurisprudential rulings, and practical applications.\n",
    "\n",
    "== INPUT ==\n",
    "You will receive:\n",
    "- 'hadith': Text of the Prophetic Hadith in Arabic (with diacritics).\n",
    "- 'explanation': Detailed explanation of the Hadith (Arabic text).\n",
    "\n",
    "== TASK ==\n",
    "Based on the provided Hadith, explanation, and your knowledge, answer the following questions in JSON format.\n",
    "\n",
    "== FIXED QUESTIONS ==\n",
    "Answer ONLY these questions without repeating the question text:\n",
    "1. What are the main messages, lessons learned, and benefits derived from the Hadith?\n",
    "2. How can the Hadith be applied in daily life?\n",
    "3. What is the importance of the Hadith in Islamic jurisprudence?\n",
    "4. What is the reason or context behind the narration of the Hadith?\n",
    "\n",
    "== RULES ==\n",
    "- Use Modern Standard Arabic with full diacritics in your answers.\n",
    "- Base your answers on the provided explanation and your knowledge of authentic Hadiths.\n",
    "- Ensure each answer accurately reflects the content and meaning of the Hadith without incorporating personal interpretations or conclusions.\n",
    "- Verify the authenticity of all information before preparing your response.\n",
    "- Return ONLY a JSON array with your answers as shown below, without any additional comments or explanations.\n",
    "\n",
    "== Expected Input ==\n",
    "{{\n",
    "  \"hadith\": \"{hadith}\",\n",
    "  \"sharh\": \"{sharh}\"\n",
    "}}\n",
    "\n",
    "== Expected Output (JSON Array) ==\n",
    "[\n",
    "  \"الإجابة الأولى مع التشكيل الكامل...\",\n",
    "  \"الإجابة الثانية مع التشكيل الكامل...\",\n",
    "  \"الإجابة الثالثة مع التشكيل الكامل...\",\n",
    "  \"الإجابة الرابعة مع التشكيل الكامل...\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def process_hadith_dataset(input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Process the hadith dataset, enriching it with QA pairs\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input JSON file\n",
    "        output_file: Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    # Load hadith dataset\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get fixed questions template\n",
    "    fixed_questions = get_fixed_questions()\n",
    "    \n",
    "    # Process each hadith with a progress bar\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(f\"Processing {len(data)} hadith entries from {input_file}...\")\n",
    "    for entry in tqdm(data, desc=f\"Processing hadiths from {os.path.basename(input_file)}\"):\n",
    "        # Skip if no sharh is available\n",
    "        if not entry.get(\"sharh\") or entry[\"sharh\"] == \".\":\n",
    "            print(f\"Skipping hadith ID {entry.get('hadith_id', 'unknown')} - no sharh available\")\n",
    "            continue\n",
    "        \n",
    "        # Create input for LLM - use the same file without creating a new one\n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            hadith=entry[\"hadith\"],\n",
    "            sharh=entry[\"sharh\"]\n",
    "        )\n",
    "        \n",
    "        # Estimate tokens before generation\n",
    "        tokens_before = estimate_tokens(prompt)\n",
    "        total_tokens_before += tokens_before\n",
    "        \n",
    "        # Stream response from Together\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-V3\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Collect response\n",
    "            output_text = \"\"\n",
    "            for token in response:\n",
    "                if hasattr(token, 'choices') and token.choices[0].delta.content:\n",
    "                    output_text += token.choices[0].delta.content\n",
    "            \n",
    "            # Estimate tokens after generation\n",
    "            tokens_after = estimate_tokens(output_text)\n",
    "            total_tokens_after += tokens_after\n",
    "            \n",
    "            # Process the output - directly match keys and values\n",
    "            try:\n",
    "                # Clean the output text - handle potential formatting issues\n",
    "                output_text = output_text.strip()\n",
    "                if output_text.startswith(\"```json\"):\n",
    "                    output_text = output_text[7:]\n",
    "                if output_text.endswith(\"```\"):\n",
    "                    output_text = output_text[:-3]\n",
    "                output_text = output_text.strip()\n",
    "                \n",
    "                # Parse as regular JSON - expecting an array of 4 strings\n",
    "                answer_list = json.loads(output_text)\n",
    "                \n",
    "                if not isinstance(answer_list, list):\n",
    "                    raise ValueError(f\"Expected list of answers, got: {type(answer_list)}\")\n",
    "                \n",
    "                # Ensure we have at least 4 answers, pad with empty strings if needed\n",
    "                while len(answer_list) < 4:\n",
    "                    answer_list.append(\"\")\n",
    "                \n",
    "                # Use direct key matching instead of creating new objects\n",
    "                qa_pairs = []\n",
    "                for i, answer in enumerate(answer_list[:4]):  # Limit to first 4 answers\n",
    "                    qa_pair = {\n",
    "                        \"question\": fixed_questions[i][\"question\"],\n",
    "                        \"answer\": answer\n",
    "                    }\n",
    "                    qa_pairs.append(qa_pair)\n",
    "                \n",
    "                # Update the existing entry directly\n",
    "                entry[\"FT_Pairs\"] = qa_pairs\n",
    "                \n",
    "                # Extract first two answers directly into lessons and application\n",
    "                entry[\"hadith_lessons\"] = [answer_list[0]] if answer_list[0] else []\n",
    "                entry[\"hadith_application\"] = [answer_list[1]] if answer_list[1] else []\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"\\n[!] Processing error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "                # Set empty values for failed processing\n",
    "                entry[\"FT_Pairs\"] = []\n",
    "                entry[\"hadith_lessons\"] = []\n",
    "                entry[\"hadith_application\"] = []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] API error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "            entry[\"FT_Pairs\"] = []\n",
    "            entry[\"hadith_lessons\"] = []\n",
    "            entry[\"hadith_application\"] = []\n",
    "    \n",
    "    # Save the enriched data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nProcessing completed for file {input_file}:\")\n",
    "    print(f\"- Successfully processed: {processed_count}/{len(data)} entries\")\n",
    "    print(f\"- Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"- Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"- Token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"- Dataset saved to '{output_file}'\")\n",
    "    \n",
    "    return processed_count, total_tokens_before, total_tokens_after\n",
    "\n",
    "def process_all_hadith_files(input_dir: str, output_dir: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Process all hadith JSON files in a directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to the directory containing input JSON files\n",
    "        output_dir: Path to save the output JSON files (if None, will use input_dir with '_processed' suffix)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if output_dir is None:\n",
    "        output_dir = input_dir + \"_processed\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files in the input directory\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    total_processed = 0\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    \n",
    "    for input_file in json_files:\n",
    "        file_name = os.path.basename(input_file)\n",
    "        output_file = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        print(f\"\\nProcessing file: {file_name}\")\n",
    "        processed, tokens_before, tokens_after = process_hadith_dataset(input_file, output_file)\n",
    "        \n",
    "        total_processed += processed\n",
    "        total_tokens_before += tokens_before\n",
    "        total_tokens_after += tokens_after\n",
    "        \n",
    "        # Add a small delay between files to avoid rate limiting\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"\\n===== OVERALL PROCESSING SUMMARY =====\")\n",
    "    print(f\"Total files processed: {len(json_files)}\")\n",
    "    print(f\"Total hadiths processed: {total_processed}\")\n",
    "    print(f\"Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"Overall token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"All processed files saved to '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1899 JSON files to process\n",
      "\n",
      "Processing file: 1326.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1326.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1326.json:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1326.json: 100%|██████████| 1/1 [00:34<00:00, 34.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1626\n",
      "- Total tokens after generation: 753\n",
      "- Token reduction ratio: 2.16x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1326.json'\n",
      "\n",
      "Processing file: 1327.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1327.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1327.json: 100%|██████████| 1/1 [00:43<00:00, 43.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 927\n",
      "- Total tokens after generation: 634\n",
      "- Token reduction ratio: 1.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1327.json'\n",
      "\n",
      "Processing file: 1328.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1328.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1328.json: 100%|██████████| 1/1 [00:51<00:00, 51.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1903\n",
      "- Total tokens after generation: 889\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1328.json'\n",
      "\n",
      "Processing file: 1329.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1329.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1329.json: 100%|██████████| 1/1 [00:35<00:00, 35.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2064\n",
      "- Total tokens after generation: 655\n",
      "- Token reduction ratio: 3.15x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1329.json'\n",
      "\n",
      "Processing file: 1330.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1330.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1330.json: 100%|██████████| 1/1 [00:41<00:00, 41.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1788\n",
      "- Total tokens after generation: 728\n",
      "- Token reduction ratio: 2.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1330.json'\n",
      "\n",
      "Processing file: 1331.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1331.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1331.json: 100%|██████████| 1/1 [00:47<00:00, 47.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1458\n",
      "- Total tokens after generation: 520\n",
      "- Token reduction ratio: 2.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1331.json'\n",
      "\n",
      "Processing file: 1332.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1332.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1332.json: 100%|██████████| 1/1 [01:04<00:00, 64.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1396\n",
      "- Total tokens after generation: 758\n",
      "- Token reduction ratio: 1.84x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1332.json'\n",
      "\n",
      "Processing file: 1333.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1333.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1333.json: 100%|██████████| 1/1 [00:56<00:00, 56.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 5547\n",
      "- Total tokens after generation: 782\n",
      "- Token reduction ratio: 7.09x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1333.json'\n",
      "\n",
      "Processing file: 1334.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1334.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1334.json: 100%|██████████| 1/1 [01:07<00:00, 67.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2752\n",
      "- Total tokens after generation: 841\n",
      "- Token reduction ratio: 3.27x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1334.json'\n",
      "\n",
      "Processing file: 1335.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1335.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1335.json: 100%|██████████| 1/1 [00:52<00:00, 52.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2620\n",
      "- Total tokens after generation: 773\n",
      "- Token reduction ratio: 3.39x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1335.json'\n",
      "\n",
      "Processing file: 1336.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1336.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1336.json: 100%|██████████| 1/1 [01:06<00:00, 66.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1814\n",
      "- Total tokens after generation: 833\n",
      "- Token reduction ratio: 2.18x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1336.json'\n",
      "\n",
      "Processing file: 1337.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1337.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1337.json: 100%|██████████| 1/1 [01:40<00:00, 100.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1077\n",
      "- Total tokens after generation: 711\n",
      "- Token reduction ratio: 1.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1337.json'\n",
      "\n",
      "Processing file: 1338.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1338.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1338.json: 100%|██████████| 1/1 [01:29<00:00, 89.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1602\n",
      "- Total tokens after generation: 700\n",
      "- Token reduction ratio: 2.29x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1338.json'\n",
      "\n",
      "Processing file: 1339.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1339.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1339.json: 100%|██████████| 1/1 [01:05<00:00, 65.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1745\n",
      "- Total tokens after generation: 713\n",
      "- Token reduction ratio: 2.45x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1339.json'\n",
      "\n",
      "Processing file: 134.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\134.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 134.json: 100%|██████████| 1/1 [01:52<00:00, 112.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2284\n",
      "- Total tokens after generation: 642\n",
      "- Token reduction ratio: 3.56x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\134.json'\n",
      "\n",
      "Processing file: 1340.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1340.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1340.json: 100%|██████████| 1/1 [01:02<00:00, 62.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1787\n",
      "- Total tokens after generation: 599\n",
      "- Token reduction ratio: 2.98x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1340.json'\n",
      "\n",
      "Processing file: 1341.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1341.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1341.json: 100%|██████████| 1/1 [00:59<00:00, 59.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1784\n",
      "- Total tokens after generation: 648\n",
      "- Token reduction ratio: 2.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1341.json'\n",
      "\n",
      "Processing file: 1342.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1342.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1342.json: 100%|██████████| 1/1 [00:56<00:00, 56.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3115\n",
      "- Total tokens after generation: 718\n",
      "- Token reduction ratio: 4.34x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1342.json'\n",
      "\n",
      "Processing file: 1343.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1343.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1343.json: 100%|██████████| 1/1 [01:39<00:00, 99.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2938\n",
      "- Total tokens after generation: 897\n",
      "- Token reduction ratio: 3.28x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1343.json'\n",
      "\n",
      "Processing file: 1344.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1344.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1344.json: 100%|██████████| 1/1 [01:21<00:00, 81.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2184\n",
      "- Total tokens after generation: 914\n",
      "- Token reduction ratio: 2.39x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1344.json'\n",
      "\n",
      "Processing file: 1345.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1345.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1345.json: 100%|██████████| 1/1 [02:15<00:00, 135.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2061\n",
      "- Total tokens after generation: 884\n",
      "- Token reduction ratio: 2.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1345.json'\n",
      "\n",
      "Processing file: 1346.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1346.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1346.json: 100%|██████████| 1/1 [01:11<00:00, 71.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1939\n",
      "- Total tokens after generation: 749\n",
      "- Token reduction ratio: 2.59x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1346.json'\n",
      "\n",
      "Processing file: 1348.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1348.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1348.json: 100%|██████████| 1/1 [00:59<00:00, 59.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1636\n",
      "- Total tokens after generation: 705\n",
      "- Token reduction ratio: 2.32x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1348.json'\n",
      "\n",
      "Processing file: 1349.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1349.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1349.json: 100%|██████████| 1/1 [00:57<00:00, 57.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1425\n",
      "- Total tokens after generation: 583\n",
      "- Token reduction ratio: 2.44x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1349.json'\n",
      "\n",
      "Processing file: 135.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\135.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 135.json: 100%|██████████| 1/1 [01:02<00:00, 62.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1775\n",
      "- Total tokens after generation: 611\n",
      "- Token reduction ratio: 2.91x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\135.json'\n",
      "\n",
      "Processing file: 1350.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1350.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1350.json: 100%|██████████| 1/1 [01:10<00:00, 70.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1431\n",
      "- Total tokens after generation: 621\n",
      "- Token reduction ratio: 2.30x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1350.json'\n",
      "\n",
      "Processing file: 1351.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1351.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1351.json: 100%|██████████| 1/1 [01:04<00:00, 64.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2235\n",
      "- Total tokens after generation: 681\n",
      "- Token reduction ratio: 3.28x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1351.json'\n",
      "\n",
      "Processing file: 1352.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1352.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1352.json: 100%|██████████| 1/1 [00:51<00:00, 51.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1425\n",
      "- Total tokens after generation: 682\n",
      "- Token reduction ratio: 2.09x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1352.json'\n",
      "\n",
      "Processing file: 1353.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1353.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1353.json: 100%|██████████| 1/1 [00:35<00:00, 35.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3345\n",
      "- Total tokens after generation: 580\n",
      "- Token reduction ratio: 5.77x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1353.json'\n",
      "\n",
      "Processing file: 1354.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1354.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1354.json: 100%|██████████| 1/1 [01:29<00:00, 90.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1323\n",
      "- Total tokens after generation: 692\n",
      "- Token reduction ratio: 1.91x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1354.json'\n",
      "\n",
      "Processing file: 1356.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1356.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1356.json: 100%|██████████| 1/1 [00:54<00:00, 54.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1268\n",
      "- Total tokens after generation: 588\n",
      "- Token reduction ratio: 2.16x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1356.json'\n",
      "\n",
      "Processing file: 1357.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1357.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1357.json: 100%|██████████| 1/1 [01:18<00:00, 78.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2289\n",
      "- Total tokens after generation: 691\n",
      "- Token reduction ratio: 3.31x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1357.json'\n",
      "\n",
      "Processing file: 1358.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1358.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1358.json: 100%|██████████| 1/1 [01:15<00:00, 75.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1382\n",
      "- Total tokens after generation: 808\n",
      "- Token reduction ratio: 1.71x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1358.json'\n",
      "\n",
      "Processing file: 1359.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1359.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1359.json: 100%|██████████| 1/1 [00:57<00:00, 57.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1166\n",
      "- Total tokens after generation: 602\n",
      "- Token reduction ratio: 1.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1359.json'\n",
      "\n",
      "Processing file: 136.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\136.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 136.json: 100%|██████████| 1/1 [01:10<00:00, 70.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1854\n",
      "- Total tokens after generation: 615\n",
      "- Token reduction ratio: 3.01x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\136.json'\n",
      "\n",
      "Processing file: 1360.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1360.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1360.json: 100%|██████████| 1/1 [00:35<00:00, 35.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1786\n",
      "- Total tokens after generation: 681\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1360.json'\n",
      "\n",
      "Processing file: 1361.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1361.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1361.json: 100%|██████████| 1/1 [01:32<00:00, 92.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2230\n",
      "- Total tokens after generation: 742\n",
      "- Token reduction ratio: 3.01x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1361.json'\n",
      "\n",
      "Processing file: 1362.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1362.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1362.json: 100%|██████████| 1/1 [01:01<00:00, 61.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1689\n",
      "- Total tokens after generation: 725\n",
      "- Token reduction ratio: 2.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1362.json'\n",
      "\n",
      "Processing file: 1363.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1363.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1363.json: 100%|██████████| 1/1 [00:33<00:00, 33.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2620\n",
      "- Total tokens after generation: 569\n",
      "- Token reduction ratio: 4.60x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1363.json'\n",
      "\n",
      "Processing file: 1364.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1364.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1364.json: 100%|██████████| 1/1 [00:59<00:00, 59.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1717\n",
      "- Total tokens after generation: 563\n",
      "- Token reduction ratio: 3.05x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1364.json'\n",
      "\n",
      "Processing file: 1365.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1365.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1365.json: 100%|██████████| 1/1 [00:39<00:00, 39.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3613\n",
      "- Total tokens after generation: 629\n",
      "- Token reduction ratio: 5.74x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1365.json'\n",
      "\n",
      "Processing file: 1366.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1366.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1366.json: 100%|██████████| 1/1 [00:46<00:00, 46.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 981\n",
      "- Total tokens after generation: 572\n",
      "- Token reduction ratio: 1.72x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1366.json'\n",
      "\n",
      "Processing file: 1368.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1368.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1368.json: 100%|██████████| 1/1 [00:26<00:00, 26.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1308\n",
      "- Total tokens after generation: 456\n",
      "- Token reduction ratio: 2.87x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1368.json'\n",
      "\n",
      "Processing file: 1369.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1369.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1369.json: 100%|██████████| 1/1 [00:31<00:00, 31.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1356\n",
      "- Total tokens after generation: 679\n",
      "- Token reduction ratio: 2.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1369.json'\n",
      "\n",
      "Processing file: 137.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\137.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 137.json: 100%|██████████| 1/1 [00:44<00:00, 44.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1837\n",
      "- Total tokens after generation: 721\n",
      "- Token reduction ratio: 2.55x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\137.json'\n",
      "\n",
      "Processing file: 1370.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1370.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1370.json: 100%|██████████| 1/1 [01:24<00:00, 84.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1211\n",
      "- Total tokens after generation: 611\n",
      "- Token reduction ratio: 1.98x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1370.json'\n",
      "\n",
      "Processing file: 1371.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1371.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1371.json: 100%|██████████| 1/1 [00:29<00:00, 29.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3358\n",
      "- Total tokens after generation: 666\n",
      "- Token reduction ratio: 5.04x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1371.json'\n",
      "\n",
      "Processing file: 1372.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1372.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1372.json: 100%|██████████| 1/1 [01:20<00:00, 80.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1739\n",
      "- Total tokens after generation: 737\n",
      "- Token reduction ratio: 2.36x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1372.json'\n",
      "\n",
      "Processing file: 1373.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1373.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1373.json: 100%|██████████| 1/1 [01:23<00:00, 83.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2208\n",
      "- Total tokens after generation: 736\n",
      "- Token reduction ratio: 3.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1373.json'\n",
      "\n",
      "Processing file: 1374.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1374.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1374.json: 100%|██████████| 1/1 [01:41<00:00, 101.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3974\n",
      "- Total tokens after generation: 1036\n",
      "- Token reduction ratio: 3.84x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1374.json'\n",
      "\n",
      "Processing file: 1375.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1375.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1375.json: 100%|██████████| 1/1 [01:08<00:00, 68.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1411\n",
      "- Total tokens after generation: 663\n",
      "- Token reduction ratio: 2.13x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1375.json'\n",
      "\n",
      "Processing file: 1376.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1376.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1376.json: 100%|██████████| 1/1 [01:15<00:00, 75.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3028\n",
      "- Total tokens after generation: 1105\n",
      "- Token reduction ratio: 2.74x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1376.json'\n",
      "\n",
      "Processing file: 1377.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1377.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1377.json: 100%|██████████| 1/1 [01:44<00:00, 104.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1937\n",
      "- Total tokens after generation: 931\n",
      "- Token reduction ratio: 2.08x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1377.json'\n",
      "\n",
      "Processing file: 1378.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1378.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1378.json: 100%|██████████| 1/1 [01:03<00:00, 63.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1998\n",
      "- Total tokens after generation: 719\n",
      "- Token reduction ratio: 2.78x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1378.json'\n",
      "\n",
      "Processing file: 1379.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1379.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1379.json: 100%|██████████| 1/1 [01:01<00:00, 61.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1886\n",
      "- Total tokens after generation: 846\n",
      "- Token reduction ratio: 2.23x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1379.json'\n",
      "\n",
      "Processing file: 138.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\138.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 138.json: 100%|██████████| 1/1 [02:00<00:00, 120.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3007\n",
      "- Total tokens after generation: 1127\n",
      "- Token reduction ratio: 2.67x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\138.json'\n",
      "\n",
      "Processing file: 1380.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1380.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1380.json: 100%|██████████| 1/1 [02:27<00:00, 147.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1983\n",
      "- Total tokens after generation: 756\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1380.json'\n",
      "\n",
      "Processing file: 1381.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1381.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1381.json: 100%|██████████| 1/1 [01:07<00:00, 67.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2280\n",
      "- Total tokens after generation: 759\n",
      "- Token reduction ratio: 3.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1381.json'\n",
      "\n",
      "Processing file: 1382.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1382.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1382.json: 100%|██████████| 1/1 [01:24<00:00, 84.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2036\n",
      "- Total tokens after generation: 811\n",
      "- Token reduction ratio: 2.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1382.json'\n",
      "\n",
      "Processing file: 1383.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1383.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1383.json: 100%|██████████| 1/1 [00:50<00:00, 50.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2237\n",
      "- Total tokens after generation: 612\n",
      "- Token reduction ratio: 3.66x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1383.json'\n",
      "\n",
      "Processing file: 1384.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1384.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1384.json: 100%|██████████| 1/1 [01:21<00:00, 81.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2038\n",
      "- Total tokens after generation: 624\n",
      "- Token reduction ratio: 3.27x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1384.json'\n",
      "\n",
      "Processing file: 1385.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1385.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1385.json: 100%|██████████| 1/1 [01:30<00:00, 90.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1656\n",
      "- Total tokens after generation: 675\n",
      "- Token reduction ratio: 2.45x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1385.json'\n",
      "\n",
      "Processing file: 1386.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1386.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1386.json: 100%|██████████| 1/1 [00:36<00:00, 36.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1313\n",
      "- Total tokens after generation: 627\n",
      "- Token reduction ratio: 2.09x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1386.json'\n",
      "\n",
      "Processing file: 1387.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1387.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1387.json: 100%|██████████| 1/1 [01:07<00:00, 67.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1336\n",
      "- Total tokens after generation: 556\n",
      "- Token reduction ratio: 2.40x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1387.json'\n",
      "\n",
      "Processing file: 1388.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1388.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1388.json: 100%|██████████| 1/1 [01:34<00:00, 94.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2064\n",
      "- Total tokens after generation: 820\n",
      "- Token reduction ratio: 2.52x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1388.json'\n",
      "\n",
      "Processing file: 1389.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1389.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1389.json: 100%|██████████| 1/1 [01:01<00:00, 61.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1733\n",
      "- Total tokens after generation: 623\n",
      "- Token reduction ratio: 2.78x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1389.json'\n",
      "\n",
      "Processing file: 139.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\139.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 139.json: 100%|██████████| 1/1 [01:19<00:00, 79.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2775\n",
      "- Total tokens after generation: 791\n",
      "- Token reduction ratio: 3.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\139.json'\n",
      "\n",
      "Processing file: 1390.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1390.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1390.json: 100%|██████████| 1/1 [00:52<00:00, 52.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1365\n",
      "- Total tokens after generation: 632\n",
      "- Token reduction ratio: 2.16x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1390.json'\n",
      "\n",
      "Processing file: 1391.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1391.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1391.json: 100%|██████████| 1/1 [00:52<00:00, 52.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1365\n",
      "- Total tokens after generation: 667\n",
      "- Token reduction ratio: 2.05x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1391.json'\n",
      "\n",
      "Processing file: 1392.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1392.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1392.json: 100%|██████████| 1/1 [00:55<00:00, 55.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 4247\n",
      "- Total tokens after generation: 904\n",
      "- Token reduction ratio: 4.70x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1392.json'\n",
      "\n",
      "Processing file: 1393.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1393.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1393.json: 100%|██████████| 1/1 [01:17<00:00, 77.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3175\n",
      "- Total tokens after generation: 1156\n",
      "- Token reduction ratio: 2.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1393.json'\n",
      "\n",
      "Processing file: 1394.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1394.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1394.json: 100%|██████████| 1/1 [01:02<00:00, 62.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1319\n",
      "- Total tokens after generation: 682\n",
      "- Token reduction ratio: 1.93x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1394.json'\n",
      "\n",
      "Processing file: 1395.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1395.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1395.json: 100%|██████████| 1/1 [00:42<00:00, 42.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1319\n",
      "- Total tokens after generation: 597\n",
      "- Token reduction ratio: 2.21x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1395.json'\n",
      "\n",
      "Processing file: 1396.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1396.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1396.json: 100%|██████████| 1/1 [00:35<00:00, 35.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1542\n",
      "- Total tokens after generation: 702\n",
      "- Token reduction ratio: 2.20x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1396.json'\n",
      "\n",
      "Processing file: 1397.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1397.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1397.json: 100%|██████████| 1/1 [01:16<00:00, 76.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2844\n",
      "- Total tokens after generation: 864\n",
      "- Token reduction ratio: 3.29x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1397.json'\n",
      "\n",
      "Processing file: 1398.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1398.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1398.json: 100%|██████████| 1/1 [00:53<00:00, 53.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1872\n",
      "- Total tokens after generation: 634\n",
      "- Token reduction ratio: 2.95x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1398.json'\n",
      "\n",
      "Processing file: 1399.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1399.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1399.json: 100%|██████████| 1/1 [01:15<00:00, 75.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1493\n",
      "- Total tokens after generation: 683\n",
      "- Token reduction ratio: 2.19x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1399.json'\n",
      "\n",
      "Processing file: 14.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\14.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 14.json: 100%|██████████| 1/1 [01:20<00:00, 80.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2571\n",
      "- Total tokens after generation: 857\n",
      "- Token reduction ratio: 3.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\14.json'\n",
      "\n",
      "Processing file: 140.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\140.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 140.json: 100%|██████████| 1/1 [01:09<00:00, 69.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2139\n",
      "- Total tokens after generation: 776\n",
      "- Token reduction ratio: 2.76x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\140.json'\n",
      "\n",
      "Processing file: 1400.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1400.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1400.json: 100%|██████████| 1/1 [01:06<00:00, 66.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 911\n",
      "- Total tokens after generation: 576\n",
      "- Token reduction ratio: 1.58x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1400.json'\n",
      "\n",
      "Processing file: 1401.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1401.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1401.json: 100%|██████████| 1/1 [01:21<00:00, 81.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] Processing error for hadith ID 1401: Unterminated string starting at: line 5 column 3 (char 1340)\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 2082\n",
      "- Total tokens after generation: 825\n",
      "- Token reduction ratio: 2.52x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1401.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1402.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1402.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1402.json: 100%|██████████| 1/1 [01:19<00:00, 79.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1305\n",
      "- Total tokens after generation: 669\n",
      "- Token reduction ratio: 1.95x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1402.json'\n",
      "\n",
      "Processing file: 1403.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1403.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1403.json: 100%|██████████| 1/1 [01:14<00:00, 74.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2230\n",
      "- Total tokens after generation: 852\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1403.json'\n",
      "\n",
      "Processing file: 1405.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1405.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1405.json: 100%|██████████| 1/1 [00:57<00:00, 57.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1652\n",
      "- Total tokens after generation: 646\n",
      "- Token reduction ratio: 2.56x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1405.json'\n",
      "\n",
      "Processing file: 1406.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1406.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1406.json: 100%|██████████| 1/1 [01:32<00:00, 92.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2060\n",
      "- Total tokens after generation: 828\n",
      "- Token reduction ratio: 2.49x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1406.json'\n",
      "\n",
      "Processing file: 1407.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1407.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1407.json: 100%|██████████| 1/1 [01:11<00:00, 71.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1536\n",
      "- Total tokens after generation: 618\n",
      "- Token reduction ratio: 2.49x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1407.json'\n",
      "\n",
      "Processing file: 1408.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1408.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1408.json: 100%|██████████| 1/1 [01:07<00:00, 67.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1697\n",
      "- Total tokens after generation: 792\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1408.json'\n",
      "\n",
      "Processing file: 1409.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1409.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1409.json: 100%|██████████| 1/1 [00:58<00:00, 58.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1407\n",
      "- Total tokens after generation: 651\n",
      "- Token reduction ratio: 2.16x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1409.json'\n",
      "\n",
      "Processing file: 141.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\141.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 141.json: 100%|██████████| 1/1 [01:04<00:00, 64.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1302\n",
      "- Total tokens after generation: 552\n",
      "- Token reduction ratio: 2.36x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\141.json'\n",
      "\n",
      "Processing file: 1411.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1411.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1411.json: 100%|██████████| 1/1 [01:11<00:00, 71.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1602\n",
      "- Total tokens after generation: 577\n",
      "- Token reduction ratio: 2.78x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1411.json'\n",
      "\n",
      "Processing file: 1415.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1415.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1415.json: 100%|██████████| 1/1 [01:19<00:00, 79.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1542\n",
      "- Total tokens after generation: 569\n",
      "- Token reduction ratio: 2.71x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1415.json'\n",
      "\n",
      "Processing file: 1416.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1416.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1416.json: 100%|██████████| 1/1 [01:18<00:00, 78.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1616\n",
      "- Total tokens after generation: 639\n",
      "- Token reduction ratio: 2.53x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1416.json'\n",
      "\n",
      "Processing file: 1417.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1417.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1417.json: 100%|██████████| 1/1 [00:37<00:00, 37.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1540\n",
      "- Total tokens after generation: 565\n",
      "- Token reduction ratio: 2.73x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1417.json'\n",
      "\n",
      "Processing file: 1419.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1419.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1419.json: 100%|██████████| 1/1 [00:57<00:00, 57.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1348\n",
      "- Total tokens after generation: 629\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1419.json'\n",
      "\n",
      "Processing file: 142.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\142.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 142.json: 100%|██████████| 1/1 [00:53<00:00, 53.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1579\n",
      "- Total tokens after generation: 767\n",
      "- Token reduction ratio: 2.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\142.json'\n",
      "\n",
      "Processing file: 1420.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1420.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1420.json: 100%|██████████| 1/1 [00:43<00:00, 43.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1660\n",
      "- Total tokens after generation: 565\n",
      "- Token reduction ratio: 2.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1420.json'\n",
      "\n",
      "Processing file: 1421.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1421.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1421.json: 100%|██████████| 1/1 [01:08<00:00, 68.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1574\n",
      "- Total tokens after generation: 781\n",
      "- Token reduction ratio: 2.02x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1421.json'\n",
      "\n",
      "Processing file: 1424.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1424.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1424.json: 100%|██████████| 1/1 [00:50<00:00, 50.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2089\n",
      "- Total tokens after generation: 719\n",
      "- Token reduction ratio: 2.91x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1424.json'\n",
      "\n",
      "Processing file: 1425.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1425.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1425.json: 100%|██████████| 1/1 [00:51<00:00, 51.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2761\n",
      "- Total tokens after generation: 736\n",
      "- Token reduction ratio: 3.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1425.json'\n",
      "\n",
      "Processing file: 1426.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1426.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1426.json: 100%|██████████| 1/1 [00:56<00:00, 56.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1574\n",
      "- Total tokens after generation: 632\n",
      "- Token reduction ratio: 2.49x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1426.json'\n",
      "\n",
      "Processing file: 1427.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1427.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1427.json: 100%|██████████| 1/1 [01:52<00:00, 112.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2173\n",
      "- Total tokens after generation: 763\n",
      "- Token reduction ratio: 2.85x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1427.json'\n",
      "\n",
      "Processing file: 1428.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1428.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1428.json: 100%|██████████| 1/1 [01:15<00:00, 75.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 4171\n",
      "- Total tokens after generation: 866\n",
      "- Token reduction ratio: 4.82x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1428.json'\n",
      "\n",
      "Processing file: 143.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\143.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 143.json: 100%|██████████| 1/1 [01:18<00:00, 78.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3726\n",
      "- Total tokens after generation: 925\n",
      "- Token reduction ratio: 4.03x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\143.json'\n",
      "\n",
      "Processing file: 1431.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1431.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1431.json: 100%|██████████| 1/1 [01:13<00:00, 73.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1466\n",
      "- Total tokens after generation: 622\n",
      "- Token reduction ratio: 2.36x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1431.json'\n",
      "\n",
      "Processing file: 1433.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1433.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1433.json: 100%|██████████| 1/1 [01:03<00:00, 63.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2758\n",
      "- Total tokens after generation: 829\n",
      "- Token reduction ratio: 3.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1433.json'\n",
      "\n",
      "Processing file: 1434.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1434.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1434.json: 100%|██████████| 1/1 [01:36<00:00, 96.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1492\n",
      "- Total tokens after generation: 665\n",
      "- Token reduction ratio: 2.24x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1434.json'\n",
      "\n",
      "Processing file: 1435.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1435.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1435.json: 100%|██████████| 1/1 [01:21<00:00, 81.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1438\n",
      "- Total tokens after generation: 663\n",
      "- Token reduction ratio: 2.17x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1435.json'\n",
      "\n",
      "Processing file: 1436.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1436.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1436.json: 100%|██████████| 1/1 [01:22<00:00, 82.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1391\n",
      "- Total tokens after generation: 611\n",
      "- Token reduction ratio: 2.28x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1436.json'\n",
      "\n",
      "Processing file: 1437.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1437.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1437.json: 100%|██████████| 1/1 [00:59<00:00, 59.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1572\n",
      "- Total tokens after generation: 579\n",
      "- Token reduction ratio: 2.72x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1437.json'\n",
      "\n",
      "Processing file: 1438.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1438.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1438.json: 100%|██████████| 1/1 [01:05<00:00, 65.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2154\n",
      "- Total tokens after generation: 614\n",
      "- Token reduction ratio: 3.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1438.json'\n",
      "\n",
      "Processing file: 1439.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1439.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1439.json: 100%|██████████| 1/1 [01:38<00:00, 98.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1778\n",
      "- Total tokens after generation: 636\n",
      "- Token reduction ratio: 2.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1439.json'\n",
      "\n",
      "Processing file: 144.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\144.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 144.json: 100%|██████████| 1/1 [01:54<00:00, 114.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2993\n",
      "- Total tokens after generation: 1115\n",
      "- Token reduction ratio: 2.68x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\144.json'\n",
      "\n",
      "Processing file: 1440.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1440.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1440.json: 100%|██████████| 1/1 [00:59<00:00, 59.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1292\n",
      "- Total tokens after generation: 533\n",
      "- Token reduction ratio: 2.42x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1440.json'\n",
      "\n",
      "Processing file: 1441.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1441.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1441.json: 100%|██████████| 1/1 [01:30<00:00, 90.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1942\n",
      "- Total tokens after generation: 687\n",
      "- Token reduction ratio: 2.83x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1441.json'\n",
      "\n",
      "Processing file: 1442.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1442.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1442.json: 100%|██████████| 1/1 [01:57<00:00, 117.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1799\n",
      "- Total tokens after generation: 772\n",
      "- Token reduction ratio: 2.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1442.json'\n",
      "\n",
      "Processing file: 1443.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1443.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1443.json: 100%|██████████| 1/1 [01:29<00:00, 89.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1601\n",
      "- Total tokens after generation: 707\n",
      "- Token reduction ratio: 2.26x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1443.json'\n",
      "\n",
      "Processing file: 1444.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1444.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1444.json: 100%|██████████| 1/1 [01:07<00:00, 67.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1731\n",
      "- Total tokens after generation: 593\n",
      "- Token reduction ratio: 2.92x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1444.json'\n",
      "\n",
      "Processing file: 1445.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1445.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1445.json: 100%|██████████| 1/1 [00:56<00:00, 56.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1713\n",
      "- Total tokens after generation: 550\n",
      "- Token reduction ratio: 3.11x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1445.json'\n",
      "\n",
      "Processing file: 1449.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1449.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1449.json: 100%|██████████| 1/1 [01:02<00:00, 62.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2719\n",
      "- Total tokens after generation: 711\n",
      "- Token reduction ratio: 3.82x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1449.json'\n",
      "\n",
      "Processing file: 145.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\145.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 145.json: 100%|██████████| 1/1 [01:36<00:00, 96.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1866\n",
      "- Total tokens after generation: 715\n",
      "- Token reduction ratio: 2.61x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\145.json'\n",
      "\n",
      "Processing file: 1450.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1450.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1450.json: 100%|██████████| 1/1 [01:28<00:00, 88.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1403\n",
      "- Total tokens after generation: 548\n",
      "- Token reduction ratio: 2.56x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1450.json'\n",
      "\n",
      "Processing file: 1451.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1451.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1451.json: 100%|██████████| 1/1 [01:17<00:00, 77.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1382\n",
      "- Total tokens after generation: 657\n",
      "- Token reduction ratio: 2.10x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1451.json'\n",
      "\n",
      "Processing file: 1452.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1452.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1452.json: 100%|██████████| 1/1 [00:42<00:00, 42.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1220\n",
      "- Total tokens after generation: 485\n",
      "- Token reduction ratio: 2.52x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1452.json'\n",
      "\n",
      "Processing file: 1453.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1453.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1453.json: 100%|██████████| 1/1 [00:47<00:00, 47.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2308\n",
      "- Total tokens after generation: 787\n",
      "- Token reduction ratio: 2.93x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1453.json'\n",
      "\n",
      "Processing file: 1454.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1454.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1454.json: 100%|██████████| 1/1 [01:08<00:00, 68.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2169\n",
      "- Total tokens after generation: 733\n",
      "- Token reduction ratio: 2.96x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1454.json'\n",
      "\n",
      "Processing file: 1456.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1456.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1456.json: 100%|██████████| 1/1 [00:47<00:00, 47.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1898\n",
      "- Total tokens after generation: 683\n",
      "- Token reduction ratio: 2.78x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1456.json'\n",
      "\n",
      "Processing file: 1457.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1457.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1457.json: 100%|██████████| 1/1 [00:54<00:00, 54.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2179\n",
      "- Total tokens after generation: 786\n",
      "- Token reduction ratio: 2.77x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1457.json'\n",
      "\n",
      "Processing file: 1458.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1458.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1458.json: 100%|██████████| 1/1 [00:50<00:00, 50.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1827\n",
      "- Total tokens after generation: 818\n",
      "- Token reduction ratio: 2.23x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1458.json'\n",
      "\n",
      "Processing file: 1459.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1459.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1459.json: 100%|██████████| 1/1 [01:12<00:00, 72.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1582\n",
      "- Total tokens after generation: 781\n",
      "- Token reduction ratio: 2.03x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1459.json'\n",
      "\n",
      "Processing file: 146.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\146.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 146.json: 100%|██████████| 1/1 [00:39<00:00, 39.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1893\n",
      "- Total tokens after generation: 689\n",
      "- Token reduction ratio: 2.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\146.json'\n",
      "\n",
      "Processing file: 1460.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1460.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1460.json: 100%|██████████| 1/1 [00:51<00:00, 52.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1416\n",
      "- Total tokens after generation: 665\n",
      "- Token reduction ratio: 2.13x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1460.json'\n",
      "\n",
      "Processing file: 1461.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1461.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1461.json: 100%|██████████| 1/1 [00:54<00:00, 54.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1811\n",
      "- Total tokens after generation: 712\n",
      "- Token reduction ratio: 2.54x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1461.json'\n",
      "\n",
      "Processing file: 1462.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1462.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1462.json: 100%|██████████| 1/1 [01:06<00:00, 66.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2960\n",
      "- Total tokens after generation: 819\n",
      "- Token reduction ratio: 3.61x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1462.json'\n",
      "\n",
      "Processing file: 1463.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1463.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1463.json: 100%|██████████| 1/1 [00:38<00:00, 38.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1820\n",
      "- Total tokens after generation: 726\n",
      "- Token reduction ratio: 2.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1463.json'\n",
      "\n",
      "Processing file: 1464.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1464.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1464.json: 100%|██████████| 1/1 [00:56<00:00, 56.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1904\n",
      "- Total tokens after generation: 934\n",
      "- Token reduction ratio: 2.04x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1464.json'\n",
      "\n",
      "Processing file: 1467.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1467.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1467.json: 100%|██████████| 1/1 [00:59<00:00, 59.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1277\n",
      "- Total tokens after generation: 675\n",
      "- Token reduction ratio: 1.89x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1467.json'\n",
      "\n",
      "Processing file: 1468.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1468.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1468.json: 100%|██████████| 1/1 [00:35<00:00, 35.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2526\n",
      "- Total tokens after generation: 703\n",
      "- Token reduction ratio: 3.59x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1468.json'\n",
      "\n",
      "Processing file: 1469.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1469.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1469.json: 100%|██████████| 1/1 [00:46<00:00, 46.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1380\n",
      "- Total tokens after generation: 794\n",
      "- Token reduction ratio: 1.74x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1469.json'\n",
      "\n",
      "Processing file: 147.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\147.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 147.json: 100%|██████████| 1/1 [00:26<00:00, 26.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 884\n",
      "- Total tokens after generation: 450\n",
      "- Token reduction ratio: 1.96x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\147.json'\n",
      "\n",
      "Processing file: 1470.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1470.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1470.json: 100%|██████████| 1/1 [00:46<00:00, 46.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1651\n",
      "- Total tokens after generation: 785\n",
      "- Token reduction ratio: 2.10x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1470.json'\n",
      "\n",
      "Processing file: 1471.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1471.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1471.json: 100%|██████████| 1/1 [00:45<00:00, 45.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3226\n",
      "- Total tokens after generation: 999\n",
      "- Token reduction ratio: 3.23x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1471.json'\n",
      "\n",
      "Processing file: 1472.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1472.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1472.json: 100%|██████████| 1/1 [00:43<00:00, 43.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1774\n",
      "- Total tokens after generation: 767\n",
      "- Token reduction ratio: 2.31x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1472.json'\n",
      "\n",
      "Processing file: 1473.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1473.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1473.json: 100%|██████████| 1/1 [00:33<00:00, 33.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1922\n",
      "- Total tokens after generation: 631\n",
      "- Token reduction ratio: 3.05x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1473.json'\n",
      "\n",
      "Processing file: 1474.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1474.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1474.json: 100%|██████████| 1/1 [00:41<00:00, 41.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3022\n",
      "- Total tokens after generation: 731\n",
      "- Token reduction ratio: 4.13x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1474.json'\n",
      "\n",
      "Processing file: 1475.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1475.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1475.json: 100%|██████████| 1/1 [00:56<00:00, 56.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1227\n",
      "- Total tokens after generation: 700\n",
      "- Token reduction ratio: 1.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1475.json'\n",
      "\n",
      "Processing file: 1476.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1476.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1476.json: 100%|██████████| 1/1 [00:34<00:00, 34.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2286\n",
      "- Total tokens after generation: 778\n",
      "- Token reduction ratio: 2.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1476.json'\n",
      "\n",
      "Processing file: 1477.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1477.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1477.json: 100%|██████████| 1/1 [00:38<00:00, 38.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1635\n",
      "- Total tokens after generation: 665\n",
      "- Token reduction ratio: 2.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1477.json'\n",
      "\n",
      "Processing file: 1478.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1478.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1478.json: 100%|██████████| 1/1 [01:02<00:00, 62.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 4095\n",
      "- Total tokens after generation: 891\n",
      "- Token reduction ratio: 4.60x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1478.json'\n",
      "\n",
      "Processing file: 1479.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1479.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1479.json: 100%|██████████| 1/1 [01:00<00:00, 60.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 10354\n",
      "- Total tokens after generation: 906\n",
      "- Token reduction ratio: 11.43x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1479.json'\n",
      "\n",
      "Processing file: 148.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\148.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 148.json: 100%|██████████| 1/1 [00:43<00:00, 43.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1562\n",
      "- Total tokens after generation: 610\n",
      "- Token reduction ratio: 2.56x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\148.json'\n",
      "\n",
      "Processing file: 1480.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1480.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1480.json: 100%|██████████| 1/1 [01:40<00:00, 100.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2687\n",
      "- Total tokens after generation: 860\n",
      "- Token reduction ratio: 3.12x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1480.json'\n",
      "\n",
      "Processing file: 1481.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1481.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1481.json: 100%|██████████| 1/1 [00:08<00:00,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1481: Response ended prematurely\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1684\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1481.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1482.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1482.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1482.json: 100%|██████████| 1/1 [01:18<00:00, 78.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1603\n",
      "- Total tokens after generation: 622\n",
      "- Token reduction ratio: 2.58x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1482.json'\n",
      "\n",
      "Processing file: 1483.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1483.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1483.json: 100%|██████████| 1/1 [01:09<00:00, 69.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1894\n",
      "- Total tokens after generation: 674\n",
      "- Token reduction ratio: 2.81x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1483.json'\n",
      "\n",
      "Processing file: 1484.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1484.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1484.json: 100%|██████████| 1/1 [01:15<00:00, 75.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3304\n",
      "- Total tokens after generation: 689\n",
      "- Token reduction ratio: 4.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1484.json'\n",
      "\n",
      "Processing file: 1485.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1485.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1485.json: 100%|██████████| 1/1 [00:54<00:00, 54.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3046\n",
      "- Total tokens after generation: 596\n",
      "- Token reduction ratio: 5.11x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1485.json'\n",
      "\n",
      "Processing file: 1486.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1486.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1486.json: 100%|██████████| 1/1 [01:10<00:00, 70.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 954\n",
      "- Total tokens after generation: 622\n",
      "- Token reduction ratio: 1.53x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1486.json'\n",
      "\n",
      "Processing file: 1487.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1487.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1487.json: 100%|██████████| 1/1 [00:42<00:00, 42.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 954\n",
      "- Total tokens after generation: 678\n",
      "- Token reduction ratio: 1.41x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1487.json'\n",
      "\n",
      "Processing file: 1488.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1488.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1488.json: 100%|██████████| 1/1 [01:02<00:00, 62.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2690\n",
      "- Total tokens after generation: 787\n",
      "- Token reduction ratio: 3.42x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1488.json'\n",
      "\n",
      "Processing file: 1489.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1489.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1489.json: 100%|██████████| 1/1 [01:08<00:00, 68.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2741\n",
      "- Total tokens after generation: 751\n",
      "- Token reduction ratio: 3.65x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1489.json'\n",
      "\n",
      "Processing file: 149.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\149.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 149.json: 100%|██████████| 1/1 [01:15<00:00, 75.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1478\n",
      "- Total tokens after generation: 603\n",
      "- Token reduction ratio: 2.45x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\149.json'\n",
      "\n",
      "Processing file: 1490.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1490.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1490.json: 100%|██████████| 1/1 [00:45<00:00, 45.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1009\n",
      "- Total tokens after generation: 576\n",
      "- Token reduction ratio: 1.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1490.json'\n",
      "\n",
      "Processing file: 1491.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1491.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1491.json: 100%|██████████| 1/1 [01:10<00:00, 70.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 878\n",
      "- Total tokens after generation: 624\n",
      "- Token reduction ratio: 1.41x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1491.json'\n",
      "\n",
      "Processing file: 1492.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1492.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1492.json: 100%|██████████| 1/1 [00:42<00:00, 42.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3637\n",
      "- Total tokens after generation: 666\n",
      "- Token reduction ratio: 5.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1492.json'\n",
      "\n",
      "Processing file: 1493.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1493.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1493.json: 100%|██████████| 1/1 [02:02<00:00, 122.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 4623\n",
      "- Total tokens after generation: 948\n",
      "- Token reduction ratio: 4.88x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1493.json'\n",
      "\n",
      "Processing file: 1495.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1495.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1495.json: 100%|██████████| 1/1 [01:22<00:00, 82.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3645\n",
      "- Total tokens after generation: 834\n",
      "- Token reduction ratio: 4.37x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1495.json'\n",
      "\n",
      "Processing file: 1496.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1496.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1496.json: 100%|██████████| 1/1 [01:39<00:00, 99.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2206\n",
      "- Total tokens after generation: 848\n",
      "- Token reduction ratio: 2.60x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1496.json'\n",
      "\n",
      "Processing file: 1497.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1497.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1497.json: 100%|██████████| 1/1 [01:10<00:00, 70.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3083\n",
      "- Total tokens after generation: 706\n",
      "- Token reduction ratio: 4.37x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1497.json'\n",
      "\n",
      "Processing file: 1498.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1498.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1498.json: 100%|██████████| 1/1 [00:54<00:00, 54.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2261\n",
      "- Total tokens after generation: 698\n",
      "- Token reduction ratio: 3.24x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1498.json'\n",
      "\n",
      "Processing file: 1499.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1499.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1499.json: 100%|██████████| 1/1 [00:53<00:00, 53.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1248\n",
      "- Total tokens after generation: 670\n",
      "- Token reduction ratio: 1.86x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1499.json'\n",
      "\n",
      "Processing file: 15.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\15.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 15.json: 100%|██████████| 1/1 [01:04<00:00, 64.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1439\n",
      "- Total tokens after generation: 615\n",
      "- Token reduction ratio: 2.34x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\15.json'\n",
      "\n",
      "Processing file: 150.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\150.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 150.json: 100%|██████████| 1/1 [01:17<00:00, 77.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1925\n",
      "- Total tokens after generation: 899\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\150.json'\n",
      "\n",
      "Processing file: 1500.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1500.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1500.json: 100%|██████████| 1/1 [01:14<00:00, 74.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2052\n",
      "- Total tokens after generation: 706\n",
      "- Token reduction ratio: 2.91x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1500.json'\n",
      "\n",
      "Processing file: 1502.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1502.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1502.json: 100%|██████████| 1/1 [00:45<00:00, 45.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1296\n",
      "- Total tokens after generation: 561\n",
      "- Token reduction ratio: 2.31x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1502.json'\n",
      "\n",
      "Processing file: 1504.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1504.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1504.json: 100%|██████████| 1/1 [01:44<00:00, 104.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2813\n",
      "- Total tokens after generation: 811\n",
      "- Token reduction ratio: 3.47x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1504.json'\n",
      "\n",
      "Processing file: 1505.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1505.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1505.json: 100%|██████████| 1/1 [02:44<00:00, 164.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2219\n",
      "- Total tokens after generation: 717\n",
      "- Token reduction ratio: 3.09x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1505.json'\n",
      "\n",
      "Processing file: 1506.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1506.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1506.json: 100%|██████████| 1/1 [01:00<00:00, 60.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1408\n",
      "- Total tokens after generation: 503\n",
      "- Token reduction ratio: 2.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1506.json'\n",
      "\n",
      "Processing file: 1507.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1507.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1507.json: 100%|██████████| 1/1 [00:37<00:00, 37.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 936\n",
      "- Total tokens after generation: 483\n",
      "- Token reduction ratio: 1.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1507.json'\n",
      "\n",
      "Processing file: 1508.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1508.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1508.json: 100%|██████████| 1/1 [00:43<00:00, 43.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 879\n",
      "- Total tokens after generation: 655\n",
      "- Token reduction ratio: 1.34x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1508.json'\n",
      "\n",
      "Processing file: 1509.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1509.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1509.json: 100%|██████████| 1/1 [00:46<00:00, 46.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1331\n",
      "- Total tokens after generation: 655\n",
      "- Token reduction ratio: 2.03x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1509.json'\n",
      "\n",
      "Processing file: 151.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\151.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 151.json: 100%|██████████| 1/1 [00:59<00:00, 60.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2413\n",
      "- Total tokens after generation: 820\n",
      "- Token reduction ratio: 2.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\151.json'\n",
      "\n",
      "Processing file: 1510.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1510.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1510.json: 100%|██████████| 1/1 [00:38<00:00, 38.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1494\n",
      "- Total tokens after generation: 564\n",
      "- Token reduction ratio: 2.65x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1510.json'\n",
      "\n",
      "Processing file: 1511.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1511.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1511.json: 100%|██████████| 1/1 [01:01<00:00, 61.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 971\n",
      "- Total tokens after generation: 581\n",
      "- Token reduction ratio: 1.67x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1511.json'\n",
      "\n",
      "Processing file: 1512.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1512.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1512.json: 100%|██████████| 1/1 [00:54<00:00, 54.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1027\n",
      "- Total tokens after generation: 644\n",
      "- Token reduction ratio: 1.59x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1512.json'\n",
      "\n",
      "Processing file: 1513.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1513.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1513.json: 100%|██████████| 1/1 [00:33<00:00, 33.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1705\n",
      "- Total tokens after generation: 564\n",
      "- Token reduction ratio: 3.02x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1513.json'\n",
      "\n",
      "Processing file: 1519.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1519.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1519.json: 100%|██████████| 1/1 [00:58<00:00, 58.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1385\n",
      "- Total tokens after generation: 804\n",
      "- Token reduction ratio: 1.72x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1519.json'\n",
      "\n",
      "Processing file: 152.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\152.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 152.json: 100%|██████████| 1/1 [01:01<00:00, 61.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2078\n",
      "- Total tokens after generation: 927\n",
      "- Token reduction ratio: 2.24x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\152.json'\n",
      "\n",
      "Processing file: 1521.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1521.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1521.json: 100%|██████████| 1/1 [00:53<00:00, 53.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1725\n",
      "- Total tokens after generation: 716\n",
      "- Token reduction ratio: 2.41x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1521.json'\n",
      "\n",
      "Processing file: 1524.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1524.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1524.json: 100%|██████████| 1/1 [00:48<00:00, 48.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1931\n",
      "- Total tokens after generation: 789\n",
      "- Token reduction ratio: 2.45x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1524.json'\n",
      "\n",
      "Processing file: 1525.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1525.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1525.json: 100%|██████████| 1/1 [00:42<00:00, 42.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1996\n",
      "- Total tokens after generation: 673\n",
      "- Token reduction ratio: 2.97x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1525.json'\n",
      "\n",
      "Processing file: 1526.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1526.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1526.json: 100%|██████████| 1/1 [00:36<00:00, 36.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1652\n",
      "- Total tokens after generation: 662\n",
      "- Token reduction ratio: 2.50x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1526.json'\n",
      "\n",
      "Processing file: 1527.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1527.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1527.json: 100%|██████████| 1/1 [00:24<00:00, 24.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 900\n",
      "- Total tokens after generation: 458\n",
      "- Token reduction ratio: 1.97x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1527.json'\n",
      "\n",
      "Processing file: 1528.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1528.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1528.json: 100%|██████████| 1/1 [00:38<00:00, 38.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1962\n",
      "- Total tokens after generation: 654\n",
      "- Token reduction ratio: 3.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1528.json'\n",
      "\n",
      "Processing file: 1529.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1529.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1529.json: 100%|██████████| 1/1 [00:52<00:00, 52.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1309\n",
      "- Total tokens after generation: 528\n",
      "- Token reduction ratio: 2.48x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1529.json'\n",
      "\n",
      "Processing file: 153.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\153.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 153.json: 100%|██████████| 1/1 [01:12<00:00, 72.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1708\n",
      "- Total tokens after generation: 775\n",
      "- Token reduction ratio: 2.20x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\153.json'\n",
      "\n",
      "Processing file: 1530.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1530.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1530.json: 100%|██████████| 1/1 [01:03<00:00, 63.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1742\n",
      "- Total tokens after generation: 687\n",
      "- Token reduction ratio: 2.54x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1530.json'\n",
      "\n",
      "Processing file: 1531.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1531.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1531.json: 100%|██████████| 1/1 [01:13<00:00, 73.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2018\n",
      "- Total tokens after generation: 734\n",
      "- Token reduction ratio: 2.75x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1531.json'\n",
      "\n",
      "Processing file: 1532.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1532.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1532.json: 100%|██████████| 1/1 [01:00<00:00, 60.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2048\n",
      "- Total tokens after generation: 645\n",
      "- Token reduction ratio: 3.18x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1532.json'\n",
      "\n",
      "Processing file: 1533.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1533.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1533.json: 100%|██████████| 1/1 [01:02<00:00, 62.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1675\n",
      "- Total tokens after generation: 748\n",
      "- Token reduction ratio: 2.24x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1533.json'\n",
      "\n",
      "Processing file: 1534.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1534.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1534.json: 100%|██████████| 1/1 [01:22<00:00, 82.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2045\n",
      "- Total tokens after generation: 759\n",
      "- Token reduction ratio: 2.69x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1534.json'\n",
      "\n",
      "Processing file: 1535.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1535.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1535.json: 100%|██████████| 1/1 [00:15<00:00, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1535: Response ended prematurely\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 2070\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1535.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1536.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1536.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1536.json: 100%|██████████| 1/1 [01:15<00:00, 75.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1816\n",
      "- Total tokens after generation: 566\n",
      "- Token reduction ratio: 3.21x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1536.json'\n",
      "\n",
      "Processing file: 1538.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1538.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1538.json: 100%|██████████| 1/1 [00:37<00:00, 37.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2023\n",
      "- Total tokens after generation: 764\n",
      "- Token reduction ratio: 2.65x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1538.json'\n",
      "\n",
      "Processing file: 1539.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1539.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1539.json: 100%|██████████| 1/1 [00:58<00:00, 58.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2307\n",
      "- Total tokens after generation: 603\n",
      "- Token reduction ratio: 3.83x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1539.json'\n",
      "\n",
      "Processing file: 154.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\154.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 154.json: 100%|██████████| 1/1 [01:14<00:00, 74.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2031\n",
      "- Total tokens after generation: 642\n",
      "- Token reduction ratio: 3.16x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\154.json'\n",
      "\n",
      "Processing file: 1540.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1540.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1540.json: 100%|██████████| 1/1 [00:51<00:00, 51.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 944\n",
      "- Total tokens after generation: 519\n",
      "- Token reduction ratio: 1.82x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1540.json'\n",
      "\n",
      "Processing file: 1541.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1541.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1541.json: 100%|██████████| 1/1 [00:59<00:00, 59.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1875\n",
      "- Total tokens after generation: 762\n",
      "- Token reduction ratio: 2.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1541.json'\n",
      "\n",
      "Processing file: 1542.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1542.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1542.json: 100%|██████████| 1/1 [00:43<00:00, 43.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2141\n",
      "- Total tokens after generation: 719\n",
      "- Token reduction ratio: 2.98x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1542.json'\n",
      "\n",
      "Processing file: 1543.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1543.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1543.json: 100%|██████████| 1/1 [00:36<00:00, 36.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1299\n",
      "- Total tokens after generation: 519\n",
      "- Token reduction ratio: 2.50x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1543.json'\n",
      "\n",
      "Processing file: 1544.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1544.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1544.json: 100%|██████████| 1/1 [00:47<00:00, 47.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2164\n",
      "- Total tokens after generation: 702\n",
      "- Token reduction ratio: 3.08x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1544.json'\n",
      "\n",
      "Processing file: 1545.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1545.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1545.json: 100%|██████████| 1/1 [01:18<00:00, 78.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2089\n",
      "- Total tokens after generation: 796\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1545.json'\n",
      "\n",
      "Processing file: 1546.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1546.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1546.json: 100%|██████████| 1/1 [01:23<00:00, 83.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2137\n",
      "- Total tokens after generation: 881\n",
      "- Token reduction ratio: 2.43x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1546.json'\n",
      "\n",
      "Processing file: 1547.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1547.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1547.json: 100%|██████████| 1/1 [00:34<00:00, 34.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1796\n",
      "- Total tokens after generation: 625\n",
      "- Token reduction ratio: 2.87x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1547.json'\n",
      "\n",
      "Processing file: 1548.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1548.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1548.json: 100%|██████████| 1/1 [00:47<00:00, 47.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2658\n",
      "- Total tokens after generation: 711\n",
      "- Token reduction ratio: 3.74x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1548.json'\n",
      "\n",
      "Processing file: 1549.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1549.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1549.json: 100%|██████████| 1/1 [00:39<00:00, 39.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1626\n",
      "- Total tokens after generation: 576\n",
      "- Token reduction ratio: 2.82x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1549.json'\n",
      "\n",
      "Processing file: 155.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\155.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 155.json: 100%|██████████| 1/1 [00:37<00:00, 37.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1827\n",
      "- Total tokens after generation: 825\n",
      "- Token reduction ratio: 2.21x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\155.json'\n",
      "\n",
      "Processing file: 1550.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1550.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1550.json: 100%|██████████| 1/1 [00:25<00:00, 25.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1723\n",
      "- Total tokens after generation: 578\n",
      "- Token reduction ratio: 2.98x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1550.json'\n",
      "\n",
      "Processing file: 1551.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1551.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1551.json: 100%|██████████| 1/1 [00:54<00:00, 54.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2771\n",
      "- Total tokens after generation: 921\n",
      "- Token reduction ratio: 3.01x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1551.json'\n",
      "\n",
      "Processing file: 1552.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1552.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1552.json: 100%|██████████| 1/1 [00:41<00:00, 41.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1374\n",
      "- Total tokens after generation: 661\n",
      "- Token reduction ratio: 2.08x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1552.json'\n",
      "\n",
      "Processing file: 1553.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1553.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1553.json: 100%|██████████| 1/1 [00:45<00:00, 45.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1724\n",
      "- Total tokens after generation: 740\n",
      "- Token reduction ratio: 2.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1553.json'\n",
      "\n",
      "Processing file: 1554.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1554.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1554.json: 100%|██████████| 1/1 [01:01<00:00, 61.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1391\n",
      "- Total tokens after generation: 778\n",
      "- Token reduction ratio: 1.79x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1554.json'\n",
      "\n",
      "Processing file: 1555.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1555.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1555.json: 100%|██████████| 1/1 [00:41<00:00, 41.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1426\n",
      "- Total tokens after generation: 577\n",
      "- Token reduction ratio: 2.47x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1555.json'\n",
      "\n",
      "Processing file: 1556.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1556.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1556.json: 100%|██████████| 1/1 [00:52<00:00, 52.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1791\n",
      "- Total tokens after generation: 694\n",
      "- Token reduction ratio: 2.58x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1556.json'\n",
      "\n",
      "Processing file: 1557.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1557.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1557.json: 100%|██████████| 1/1 [00:42<00:00, 42.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1974\n",
      "- Total tokens after generation: 659\n",
      "- Token reduction ratio: 3.00x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1557.json'\n",
      "\n",
      "Processing file: 1558.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1558.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1558.json: 100%|██████████| 1/1 [00:53<00:00, 53.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1852\n",
      "- Total tokens after generation: 897\n",
      "- Token reduction ratio: 2.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1558.json'\n",
      "\n",
      "Processing file: 1559.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1559.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1559.json: 100%|██████████| 1/1 [00:30<00:00, 30.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1208\n",
      "- Total tokens after generation: 507\n",
      "- Token reduction ratio: 2.38x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1559.json'\n",
      "\n",
      "Processing file: 156.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\156.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 156.json: 100%|██████████| 1/1 [00:57<00:00, 57.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2209\n",
      "- Total tokens after generation: 892\n",
      "- Token reduction ratio: 2.48x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\156.json'\n",
      "\n",
      "Processing file: 1560.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1560.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1560.json: 100%|██████████| 1/1 [00:52<00:00, 52.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1615\n",
      "- Total tokens after generation: 528\n",
      "- Token reduction ratio: 3.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1560.json'\n",
      "\n",
      "Processing file: 1563.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1563.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1563.json: 100%|██████████| 1/1 [01:01<00:00, 61.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1608\n",
      "- Total tokens after generation: 583\n",
      "- Token reduction ratio: 2.76x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1563.json'\n",
      "\n",
      "Processing file: 1565.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1565.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1565.json: 100%|██████████| 1/1 [00:46<00:00, 46.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2536\n",
      "- Total tokens after generation: 708\n",
      "- Token reduction ratio: 3.58x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1565.json'\n",
      "\n",
      "Processing file: 1566.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1566.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1566.json: 100%|██████████| 1/1 [00:50<00:00, 50.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1392\n",
      "- Total tokens after generation: 608\n",
      "- Token reduction ratio: 2.29x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1566.json'\n",
      "\n",
      "Processing file: 1567.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1567.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1567.json: 100%|██████████| 1/1 [00:58<00:00, 58.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1899\n",
      "- Total tokens after generation: 722\n",
      "- Token reduction ratio: 2.63x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1567.json'\n",
      "\n",
      "Processing file: 1568.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1568.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1568.json: 100%|██████████| 1/1 [01:14<00:00, 74.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2087\n",
      "- Total tokens after generation: 739\n",
      "- Token reduction ratio: 2.82x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1568.json'\n",
      "\n",
      "Processing file: 1569.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1569.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1569.json: 100%|██████████| 1/1 [00:52<00:00, 52.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1914\n",
      "- Total tokens after generation: 691\n",
      "- Token reduction ratio: 2.77x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1569.json'\n",
      "\n",
      "Processing file: 157.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\157.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 157.json: 100%|██████████| 1/1 [00:45<00:00, 45.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1693\n",
      "- Total tokens after generation: 646\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\157.json'\n",
      "\n",
      "Processing file: 1570.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1570.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1570.json: 100%|██████████| 1/1 [00:41<00:00, 41.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1300\n",
      "- Total tokens after generation: 591\n",
      "- Token reduction ratio: 2.20x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1570.json'\n",
      "\n",
      "Processing file: 1571.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1571.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1571.json: 100%|██████████| 1/1 [01:02<00:00, 62.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1369\n",
      "- Total tokens after generation: 696\n",
      "- Token reduction ratio: 1.97x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1571.json'\n",
      "\n",
      "Processing file: 1572.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1572.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1572.json: 100%|██████████| 1/1 [01:13<00:00, 74.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1927\n",
      "- Total tokens after generation: 761\n",
      "- Token reduction ratio: 2.53x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1572.json'\n",
      "\n",
      "Processing file: 1573.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1573.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1573.json: 100%|██████████| 1/1 [01:14<00:00, 74.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1869\n",
      "- Total tokens after generation: 773\n",
      "- Token reduction ratio: 2.42x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1573.json'\n",
      "\n",
      "Processing file: 1574.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1574.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1574.json: 100%|██████████| 1/1 [01:07<00:00, 67.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1291\n",
      "- Total tokens after generation: 759\n",
      "- Token reduction ratio: 1.70x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1574.json'\n",
      "\n",
      "Processing file: 1575.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1575.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1575.json: 100%|██████████| 1/1 [00:59<00:00, 59.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1324\n",
      "- Total tokens after generation: 644\n",
      "- Token reduction ratio: 2.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1575.json'\n",
      "\n",
      "Processing file: 1576.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1576.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1576.json: 100%|██████████| 1/1 [00:51<00:00, 51.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1344\n",
      "- Total tokens after generation: 595\n",
      "- Token reduction ratio: 2.26x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1576.json'\n",
      "\n",
      "Processing file: 1577.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1577.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1577.json: 100%|██████████| 1/1 [00:52<00:00, 52.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 902\n",
      "- Total tokens after generation: 558\n",
      "- Token reduction ratio: 1.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1577.json'\n",
      "\n",
      "Processing file: 1578.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1578.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1578.json: 100%|██████████| 1/1 [01:02<00:00, 62.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] Processing error for hadith ID 1578: Unterminated string starting at: line 4 column 3 (char 769)\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 2224\n",
      "- Total tokens after generation: 536\n",
      "- Token reduction ratio: 4.15x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1578.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 158.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\158.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 158.json: 100%|██████████| 1/1 [00:46<00:00, 46.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2328\n",
      "- Total tokens after generation: 798\n",
      "- Token reduction ratio: 2.92x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\158.json'\n",
      "\n",
      "Processing file: 1580.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1580.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1580.json: 100%|██████████| 1/1 [01:00<00:00, 60.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1580: Response ended prematurely\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1967\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1580.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1584.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1584.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1584.json: 100%|██████████| 1/1 [01:10<00:00, 70.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2406\n",
      "- Total tokens after generation: 917\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1584.json'\n",
      "\n",
      "Processing file: 1586.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1586.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1586.json: 100%|██████████| 1/1 [01:03<00:00, 63.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1934\n",
      "- Total tokens after generation: 680\n",
      "- Token reduction ratio: 2.84x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1586.json'\n",
      "\n",
      "Processing file: 1587.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1587.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1587.json: 100%|██████████| 1/1 [01:36<00:00, 96.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2382\n",
      "- Total tokens after generation: 877\n",
      "- Token reduction ratio: 2.72x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1587.json'\n",
      "\n",
      "Processing file: 1588.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1588.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1588.json: 100%|██████████| 1/1 [00:52<00:00, 52.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2349\n",
      "- Total tokens after generation: 810\n",
      "- Token reduction ratio: 2.90x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1588.json'\n",
      "\n",
      "Processing file: 1589.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1589.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1589.json: 100%|██████████| 1/1 [00:30<00:00, 30.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1532\n",
      "- Total tokens after generation: 574\n",
      "- Token reduction ratio: 2.67x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1589.json'\n",
      "\n",
      "Processing file: 159.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\159.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 159.json: 100%|██████████| 1/1 [00:40<00:00, 40.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1806\n",
      "- Total tokens after generation: 695\n",
      "- Token reduction ratio: 2.60x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\159.json'\n",
      "\n",
      "Processing file: 1593.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1593.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1593.json: 100%|██████████| 1/1 [00:54<00:00, 54.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1870\n",
      "- Total tokens after generation: 769\n",
      "- Token reduction ratio: 2.43x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1593.json'\n",
      "\n",
      "Processing file: 1594.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1594.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1594.json: 100%|██████████| 1/1 [00:40<00:00, 40.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2632\n",
      "- Total tokens after generation: 624\n",
      "- Token reduction ratio: 4.22x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1594.json'\n",
      "\n",
      "Processing file: 1595.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1595.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1595.json: 100%|██████████| 1/1 [01:00<00:00, 60.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1798\n",
      "- Total tokens after generation: 839\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1595.json'\n",
      "\n",
      "Processing file: 1596.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1596.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1596.json: 100%|██████████| 1/1 [00:56<00:00, 56.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2821\n",
      "- Total tokens after generation: 821\n",
      "- Token reduction ratio: 3.44x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1596.json'\n",
      "\n",
      "Processing file: 1599.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1599.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1599.json: 100%|██████████| 1/1 [00:45<00:00, 45.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2413\n",
      "- Total tokens after generation: 747\n",
      "- Token reduction ratio: 3.23x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1599.json'\n",
      "\n",
      "Processing file: 16.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\16.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 16.json: 100%|██████████| 1/1 [00:52<00:00, 52.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1052\n",
      "- Total tokens after generation: 670\n",
      "- Token reduction ratio: 1.57x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\16.json'\n",
      "\n",
      "Processing file: 160.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\160.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 160.json: 100%|██████████| 1/1 [01:22<00:00, 82.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 4245\n",
      "- Total tokens after generation: 1013\n",
      "- Token reduction ratio: 4.19x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\160.json'\n",
      "\n",
      "Processing file: 1601.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1601.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1601.json: 100%|██████████| 1/1 [00:51<00:00, 51.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2060\n",
      "- Total tokens after generation: 678\n",
      "- Token reduction ratio: 3.04x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1601.json'\n",
      "\n",
      "Processing file: 1603.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1603.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1603.json: 100%|██████████| 1/1 [00:55<00:00, 55.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1733\n",
      "- Total tokens after generation: 833\n",
      "- Token reduction ratio: 2.08x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1603.json'\n",
      "\n",
      "Processing file: 1604.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1604.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1604.json: 100%|██████████| 1/1 [00:50<00:00, 50.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1491\n",
      "- Total tokens after generation: 583\n",
      "- Token reduction ratio: 2.56x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1604.json'\n",
      "\n",
      "Processing file: 1606.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1606.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1606.json: 100%|██████████| 1/1 [00:48<00:00, 48.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1563\n",
      "- Total tokens after generation: 705\n",
      "- Token reduction ratio: 2.22x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1606.json'\n",
      "\n",
      "Processing file: 1608.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1608.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1608.json: 100%|██████████| 1/1 [00:43<00:00, 43.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1396\n",
      "- Total tokens after generation: 497\n",
      "- Token reduction ratio: 2.81x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1608.json'\n",
      "\n",
      "Processing file: 1609.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1609.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1609.json: 100%|██████████| 1/1 [00:59<00:00, 59.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1613\n",
      "- Total tokens after generation: 622\n",
      "- Token reduction ratio: 2.59x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1609.json'\n",
      "\n",
      "Processing file: 161.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\161.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 161.json: 100%|██████████| 1/1 [01:07<00:00, 67.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2268\n",
      "- Total tokens after generation: 754\n",
      "- Token reduction ratio: 3.01x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\161.json'\n",
      "\n",
      "Processing file: 1610.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1610.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1610.json: 100%|██████████| 1/1 [00:58<00:00, 58.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1632\n",
      "- Total tokens after generation: 745\n",
      "- Token reduction ratio: 2.19x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1610.json'\n",
      "\n",
      "Processing file: 1612.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1612.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1612.json: 100%|██████████| 1/1 [00:41<00:00, 41.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1534\n",
      "- Total tokens after generation: 720\n",
      "- Token reduction ratio: 2.13x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1612.json'\n",
      "\n",
      "Processing file: 1613.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1613.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1613.json: 100%|██████████| 1/1 [01:11<00:00, 71.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1380\n",
      "- Total tokens after generation: 495\n",
      "- Token reduction ratio: 2.79x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1613.json'\n",
      "\n",
      "Processing file: 1614.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1614.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1614.json: 100%|██████████| 1/1 [01:02<00:00, 62.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1401\n",
      "- Total tokens after generation: 779\n",
      "- Token reduction ratio: 1.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1614.json'\n",
      "\n",
      "Processing file: 1615.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1615.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1615.json: 100%|██████████| 1/1 [00:39<00:00, 39.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1366\n",
      "- Total tokens after generation: 587\n",
      "- Token reduction ratio: 2.33x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1615.json'\n",
      "\n",
      "Processing file: 1616.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1616.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1616.json: 100%|██████████| 1/1 [01:32<00:00, 92.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2212\n",
      "- Total tokens after generation: 1072\n",
      "- Token reduction ratio: 2.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1616.json'\n",
      "\n",
      "Processing file: 1618.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1618.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1618.json: 100%|██████████| 1/1 [00:56<00:00, 56.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1682\n",
      "- Total tokens after generation: 639\n",
      "- Token reduction ratio: 2.63x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1618.json'\n",
      "\n",
      "Processing file: 1619.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1619.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1619.json: 100%|██████████| 1/1 [01:06<00:00, 66.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1983\n",
      "- Total tokens after generation: 694\n",
      "- Token reduction ratio: 2.86x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1619.json'\n",
      "\n",
      "Processing file: 162.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\162.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 162.json: 100%|██████████| 1/1 [01:25<00:00, 85.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 7049\n",
      "- Total tokens after generation: 857\n",
      "- Token reduction ratio: 8.23x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\162.json'\n",
      "\n",
      "Processing file: 1620.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1620.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1620.json: 100%|██████████| 1/1 [00:59<00:00, 59.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1709\n",
      "- Total tokens after generation: 833\n",
      "- Token reduction ratio: 2.05x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1620.json'\n",
      "\n",
      "Processing file: 1621.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1621.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1621.json: 100%|██████████| 1/1 [00:51<00:00, 51.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1657\n",
      "- Total tokens after generation: 804\n",
      "- Token reduction ratio: 2.06x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1621.json'\n",
      "\n",
      "Processing file: 1622.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1622.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1622.json: 100%|██████████| 1/1 [00:40<00:00, 40.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1338\n",
      "- Total tokens after generation: 570\n",
      "- Token reduction ratio: 2.35x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1622.json'\n",
      "\n",
      "Processing file: 1625.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1625.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1625.json: 100%|██████████| 1/1 [00:58<00:00, 58.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1021\n",
      "- Total tokens after generation: 602\n",
      "- Token reduction ratio: 1.70x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1625.json'\n",
      "\n",
      "Processing file: 1626.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1626.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1626.json: 100%|██████████| 1/1 [00:52<00:00, 52.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 866\n",
      "- Total tokens after generation: 571\n",
      "- Token reduction ratio: 1.52x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1626.json'\n",
      "\n",
      "Processing file: 1627.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1627.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1627.json: 100%|██████████| 1/1 [00:41<00:00, 41.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1860\n",
      "- Total tokens after generation: 841\n",
      "- Token reduction ratio: 2.21x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1627.json'\n",
      "\n",
      "Processing file: 1628.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1628.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1628.json: 100%|██████████| 1/1 [01:26<00:00, 86.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1159\n",
      "- Total tokens after generation: 601\n",
      "- Token reduction ratio: 1.93x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1628.json'\n",
      "\n",
      "Processing file: 1629.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1629.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1629.json: 100%|██████████| 1/1 [01:09<00:00, 69.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2522\n",
      "- Total tokens after generation: 953\n",
      "- Token reduction ratio: 2.65x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1629.json'\n",
      "\n",
      "Processing file: 163.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\163.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 163.json: 100%|██████████| 1/1 [01:39<00:00, 99.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 7105\n",
      "- Total tokens after generation: 1025\n",
      "- Token reduction ratio: 6.93x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\163.json'\n",
      "\n",
      "Processing file: 1632.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1632.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1632.json: 100%|██████████| 1/1 [01:16<00:00, 76.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2280\n",
      "- Total tokens after generation: 779\n",
      "- Token reduction ratio: 2.93x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1632.json'\n",
      "\n",
      "Processing file: 1636.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1636.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1636.json: 100%|██████████| 1/1 [00:45<00:00, 45.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1923\n",
      "- Total tokens after generation: 688\n",
      "- Token reduction ratio: 2.80x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1636.json'\n",
      "\n",
      "Processing file: 1637.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1637.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1637.json: 100%|██████████| 1/1 [00:57<00:00, 57.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2723\n",
      "- Total tokens after generation: 856\n",
      "- Token reduction ratio: 3.18x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1637.json'\n",
      "\n",
      "Processing file: 1639.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1639.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1639.json: 100%|██████████| 1/1 [00:56<00:00, 56.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1767\n",
      "- Total tokens after generation: 909\n",
      "- Token reduction ratio: 1.94x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1639.json'\n",
      "\n",
      "Processing file: 164.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\164.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 164.json: 100%|██████████| 1/1 [00:54<00:00, 54.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2217\n",
      "- Total tokens after generation: 661\n",
      "- Token reduction ratio: 3.35x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\164.json'\n",
      "\n",
      "Processing file: 1640.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1640.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1640.json: 100%|██████████| 1/1 [00:36<00:00, 36.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1791\n",
      "- Total tokens after generation: 561\n",
      "- Token reduction ratio: 3.19x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1640.json'\n",
      "\n",
      "Processing file: 1642.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1642.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1642.json: 100%|██████████| 1/1 [00:53<00:00, 53.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1632\n",
      "- Total tokens after generation: 761\n",
      "- Token reduction ratio: 2.14x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1642.json'\n",
      "\n",
      "Processing file: 1644.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1644.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1644.json: 100%|██████████| 1/1 [01:54<00:00, 114.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1744\n",
      "- Total tokens after generation: 773\n",
      "- Token reduction ratio: 2.26x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1644.json'\n",
      "\n",
      "Processing file: 1647.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1647.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1647.json: 100%|██████████| 1/1 [01:19<00:00, 79.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1912\n",
      "- Total tokens after generation: 700\n",
      "- Token reduction ratio: 2.73x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1647.json'\n",
      "\n",
      "Processing file: 1649.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1649.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1649.json: 100%|██████████| 1/1 [00:55<00:00, 55.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2994\n",
      "- Total tokens after generation: 789\n",
      "- Token reduction ratio: 3.79x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1649.json'\n",
      "\n",
      "Processing file: 165.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\165.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 165.json: 100%|██████████| 1/1 [01:03<00:00, 63.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2562\n",
      "- Total tokens after generation: 804\n",
      "- Token reduction ratio: 3.19x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\165.json'\n",
      "\n",
      "Processing file: 1652.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1652.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1652.json: 100%|██████████| 1/1 [01:15<00:00, 75.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2537\n",
      "- Total tokens after generation: 825\n",
      "- Token reduction ratio: 3.08x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1652.json'\n",
      "\n",
      "Processing file: 1653.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1653.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1653.json: 100%|██████████| 1/1 [00:51<00:00, 51.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1436\n",
      "- Total tokens after generation: 545\n",
      "- Token reduction ratio: 2.63x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1653.json'\n",
      "\n",
      "Processing file: 1654.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1654.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1654.json: 100%|██████████| 1/1 [01:04<00:00, 64.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1849\n",
      "- Total tokens after generation: 835\n",
      "- Token reduction ratio: 2.21x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1654.json'\n",
      "\n",
      "Processing file: 1656.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1656.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1656.json: 100%|██████████| 1/1 [00:59<00:00, 59.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1369\n",
      "- Total tokens after generation: 607\n",
      "- Token reduction ratio: 2.26x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1656.json'\n",
      "\n",
      "Processing file: 1659.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1659.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1659.json: 100%|██████████| 1/1 [00:58<00:00, 58.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2277\n",
      "- Total tokens after generation: 664\n",
      "- Token reduction ratio: 3.43x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1659.json'\n",
      "\n",
      "Processing file: 166.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\166.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 166.json: 100%|██████████| 1/1 [00:55<00:00, 55.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2055\n",
      "- Total tokens after generation: 635\n",
      "- Token reduction ratio: 3.24x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\166.json'\n",
      "\n",
      "Processing file: 1660.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1660.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1660.json: 100%|██████████| 1/1 [01:00<00:00, 60.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1332\n",
      "- Total tokens after generation: 720\n",
      "- Token reduction ratio: 1.85x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1660.json'\n",
      "\n",
      "Processing file: 1661.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1661.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1661.json: 100%|██████████| 1/1 [01:18<00:00, 78.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2055\n",
      "- Total tokens after generation: 834\n",
      "- Token reduction ratio: 2.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1661.json'\n",
      "\n",
      "Processing file: 1664.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1664.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1664.json: 100%|██████████| 1/1 [00:54<00:00, 54.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1290\n",
      "- Total tokens after generation: 698\n",
      "- Token reduction ratio: 1.85x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1664.json'\n",
      "\n",
      "Processing file: 1665.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1665.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1665.json: 100%|██████████| 1/1 [00:57<00:00, 57.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1642\n",
      "- Total tokens after generation: 794\n",
      "- Token reduction ratio: 2.07x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1665.json'\n",
      "\n",
      "Processing file: 1666.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1666.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1666.json: 100%|██████████| 1/1 [01:29<00:00, 89.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2054\n",
      "- Total tokens after generation: 817\n",
      "- Token reduction ratio: 2.51x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1666.json'\n",
      "\n",
      "Processing file: 1667.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1667.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1667.json: 100%|██████████| 1/1 [00:56<00:00, 56.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1292\n",
      "- Total tokens after generation: 690\n",
      "- Token reduction ratio: 1.87x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1667.json'\n",
      "\n",
      "Processing file: 1668.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1668.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1668.json: 100%|██████████| 1/1 [01:09<00:00, 69.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1743\n",
      "- Total tokens after generation: 834\n",
      "- Token reduction ratio: 2.09x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1668.json'\n",
      "\n",
      "Processing file: 1669.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1669.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1669.json: 100%|██████████| 1/1 [00:53<00:00, 53.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2254\n",
      "- Total tokens after generation: 893\n",
      "- Token reduction ratio: 2.52x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1669.json'\n",
      "\n",
      "Processing file: 167.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\167.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 167.json: 100%|██████████| 1/1 [00:39<00:00, 39.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1907\n",
      "- Total tokens after generation: 792\n",
      "- Token reduction ratio: 2.41x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\167.json'\n",
      "\n",
      "Processing file: 1671.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1671.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1671.json: 100%|██████████| 1/1 [00:35<00:00, 35.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2642\n",
      "- Total tokens after generation: 592\n",
      "- Token reduction ratio: 4.46x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1671.json'\n",
      "\n",
      "Processing file: 1672.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1672.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1672.json: 100%|██████████| 1/1 [01:38<00:00, 98.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1765\n",
      "- Total tokens after generation: 747\n",
      "- Token reduction ratio: 2.36x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1672.json'\n",
      "\n",
      "Processing file: 1673.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1673.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1673.json: 100%|██████████| 1/1 [00:44<00:00, 44.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1040\n",
      "- Total tokens after generation: 620\n",
      "- Token reduction ratio: 1.68x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1673.json'\n",
      "\n",
      "Processing file: 1674.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1674.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1674.json: 100%|██████████| 1/1 [00:39<00:00, 39.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1062\n",
      "- Total tokens after generation: 597\n",
      "- Token reduction ratio: 1.78x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1674.json'\n",
      "\n",
      "Processing file: 1675.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1675.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1675.json: 100%|██████████| 1/1 [01:47<00:00, 107.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2035\n",
      "- Total tokens after generation: 871\n",
      "- Token reduction ratio: 2.34x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1675.json'\n",
      "\n",
      "Processing file: 1676.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1676.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1676.json: 100%|██████████| 1/1 [00:46<00:00, 46.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1516\n",
      "- Total tokens after generation: 646\n",
      "- Token reduction ratio: 2.35x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1676.json'\n",
      "\n",
      "Processing file: 1678.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1678.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1678.json: 100%|██████████| 1/1 [00:43<00:00, 43.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1199\n",
      "- Total tokens after generation: 633\n",
      "- Token reduction ratio: 1.89x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1678.json'\n",
      "\n",
      "Processing file: 1679.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1679.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1679.json: 100%|██████████| 1/1 [00:44<00:00, 44.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 3866\n",
      "- Total tokens after generation: 709\n",
      "- Token reduction ratio: 5.45x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1679.json'\n",
      "\n",
      "Processing file: 168.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\168.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 168.json: 100%|██████████| 1/1 [01:14<00:00, 74.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1903\n",
      "- Total tokens after generation: 727\n",
      "- Token reduction ratio: 2.62x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\168.json'\n",
      "\n",
      "Processing file: 1680.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1680.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1680.json: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1680: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 3944\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1680.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1681.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1681.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1681.json: 100%|██████████| 1/1 [00:25<00:00, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] Processing error for hadith ID 1681: Unterminated string starting at: line 3 column 3 (char 380)\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1961\n",
      "- Total tokens after generation: 227\n",
      "- Token reduction ratio: 8.64x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1681.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1682.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1682.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1682.json: 100%|██████████| 1/1 [00:51<00:00, 51.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 1845\n",
      "- Total tokens after generation: 768\n",
      "- Token reduction ratio: 2.40x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1682.json'\n",
      "\n",
      "Processing file: 1684.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1684.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1684.json: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1684: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1287\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1684.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1687.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1687.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1687.json: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1687: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1495\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1687.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1688.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1688.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1688.json: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1688: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1251\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1688.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1689.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1689.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1689.json: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1689: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1686\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1689.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 169.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\169.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 169.json: 100%|██████████| 1/1 [00:36<00:00, 36.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 1/1 entries\n",
      "- Total tokens before generation: 2415\n",
      "- Total tokens after generation: 646\n",
      "- Token reduction ratio: 3.74x\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\169.json'\n",
      "\n",
      "Processing file: 1691.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1691.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1691.json: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[!] API error for hadith ID 1691: Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to add credit or upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "\n",
      "Processing completed for file {input_file}:\n",
      "- Successfully processed: 0/1 entries\n",
      "- Total tokens before generation: 1751\n",
      "- Total tokens after generation: 0\n",
      "- Dataset saved to 'd:\\زينب\\Sahih_muslim_processed\\1691.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: 1692.json\n",
      "Processing 1 hadith entries from d:\\زينب\\Sahih_muslim\\1692.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hadiths from 1692.json:   0%|          | 0/1 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 268\u001b[0m\n\u001b[0;32m    265\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mزينب\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSahih_muslim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mزينب\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSahih_muslim_processed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mprocess_all_hadith_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 244\u001b[0m, in \u001b[0;36mprocess_all_hadith_files\u001b[1;34m(input_dir, output_dir)\u001b[0m\n\u001b[0;32m    241\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, file_name)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 244\u001b[0m processed, tokens_before, tokens_after \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_hadith_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m total_processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m processed\n\u001b[0;32m    247\u001b[0m total_tokens_before \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokens_before\n",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m, in \u001b[0;36mprocess_hadith_dataset\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Collect response\u001b[39;00m\n\u001b[0;32m    136\u001b[0m output_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    139\u001b[0m         output_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\together\\resources\\chat\\completions.py:153\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ChatCompletionChunk(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mline\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatCompletionResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\together\\abstract\\api_requestor.py:627\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    621\u001b[0m content_type \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/event-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content_type:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    626\u001b[0m         )\n\u001b[1;32m--> 627\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    628\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/octet-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio/wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio/mpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\together\\abstract\\api_requestor.py:91\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(rbody)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_stream\u001b[39m(rbody: Iterator[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m rbody:\n\u001b[0;32m     92\u001b[0m         _line \u001b[38;5;241m=\u001b[39m parse_stream_helper(line)\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _line \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[0;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[0;32m    867\u001b[0m ):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files in the Sahih_muslim directory\n",
    "    input_dir = r\"d:\\زينب\\Sahih_muslim\"\n",
    "    output_dir = r\"d:\\زينب\\Sahih_muslim_processed\"\n",
    "    \n",
    "    process_all_hadith_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the mattan's hadith to questions answers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "معالجة الملفات: 100%|██████████| 656/656 [00:07<00:00, 93.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تحديث جميع ملفات الحديث بنجاح!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # مكتبة شريط التقدم\n",
    "\n",
    "# مسار المجلد الذي يحتوي على ملفات JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# الحصول على قائمة ملفات JSON\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# التكرار مع tqdm لعرض شريط التقدم\n",
    "for filename in tqdm(json_files, desc=\"معالجة الملفات\"):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # فتح ملف البيانات للقراءة\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # معالجة كل عنصر في البيانات\n",
    "    for entry in data:\n",
    "        hadith_text = entry.get(\"hadith\", \"\")\n",
    "\n",
    "        # تحديث الأسئلة في FT_Pairs\n",
    "        for pair in entry.get(\"FT_Pairs\", []):\n",
    "            if \"question\" in pair:\n",
    "                pair[\"question\"] = pair[\"question\"].replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "\n",
    "        # تحديث hadith_lessons و hadith_application إذا كانت موجودة\n",
    "        if \"hadith_lessons\" in entry and entry[\"hadith_lessons\"]:\n",
    "            for i, lesson in enumerate(entry[\"hadith_lessons\"]):\n",
    "                entry[\"hadith_lessons\"][i] = lesson.replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "        \n",
    "        if \"hadith_application\" in entry and entry[\"hadith_application\"]:\n",
    "            for i, app in enumerate(entry[\"hadith_application\"]):\n",
    "                entry[\"hadith_application\"][i] = app.replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "\n",
    "    # حفظ البيانات المعدلة\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ تم تحديث جميع ملفات الحديث بنجاح!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Empty JSON Files to Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1401.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1481.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1535.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1578.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1580.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1680.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1681.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1684.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1687.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1688.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1689.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 1691.json\n",
      "🗑️ تم حذف الملف لأن جميع الحقول فارغة: 3.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# مجلد ملفات JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# التكرار على جميع ملفات JSON في المجلد\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # فتح وتحميل الملف\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # التأكد أن الملف يحتوي على قائمة من العناصر\n",
    "            if isinstance(data, list):\n",
    "                all_empty = True\n",
    "                for entry in data:\n",
    "                    # فحص الحقول الثلاثة\n",
    "                    if (entry.get(\"hadith_lessons\") or \n",
    "                        entry.get(\"hadith_application\") or \n",
    "                        entry.get(\"FT_Pairs\")):\n",
    "                        all_empty = False\n",
    "                        break\n",
    "\n",
    "                # حذف الملف إذا كانت كل الحقول في كل العناصر فارغة\n",
    "                if all_empty:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"🗑️ تم حذف الملف لأن جميع الحقول فارغة: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ خطأ في قراءة الملف {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Processed JSON Files from Original Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ تم حذف الملف: 1326.json\n",
      "🗑️ تم حذف الملف: 1327.json\n",
      "🗑️ تم حذف الملف: 1328.json\n",
      "🗑️ تم حذف الملف: 1329.json\n",
      "🗑️ تم حذف الملف: 1330.json\n",
      "🗑️ تم حذف الملف: 1331.json\n",
      "🗑️ تم حذف الملف: 1332.json\n",
      "🗑️ تم حذف الملف: 1333.json\n",
      "🗑️ تم حذف الملف: 1334.json\n",
      "🗑️ تم حذف الملف: 1335.json\n",
      "🗑️ تم حذف الملف: 1336.json\n",
      "🗑️ تم حذف الملف: 1337.json\n",
      "🗑️ تم حذف الملف: 1338.json\n",
      "🗑️ تم حذف الملف: 1339.json\n",
      "🗑️ تم حذف الملف: 134.json\n",
      "🗑️ تم حذف الملف: 1340.json\n",
      "🗑️ تم حذف الملف: 1341.json\n",
      "🗑️ تم حذف الملف: 1342.json\n",
      "🗑️ تم حذف الملف: 1343.json\n",
      "🗑️ تم حذف الملف: 1344.json\n",
      "🗑️ تم حذف الملف: 1345.json\n",
      "🗑️ تم حذف الملف: 1346.json\n",
      "🗑️ تم حذف الملف: 1348.json\n",
      "🗑️ تم حذف الملف: 1349.json\n",
      "🗑️ تم حذف الملف: 135.json\n",
      "🗑️ تم حذف الملف: 1350.json\n",
      "🗑️ تم حذف الملف: 1351.json\n",
      "🗑️ تم حذف الملف: 1352.json\n",
      "🗑️ تم حذف الملف: 1353.json\n",
      "🗑️ تم حذف الملف: 1354.json\n",
      "🗑️ تم حذف الملف: 1356.json\n",
      "🗑️ تم حذف الملف: 1357.json\n",
      "🗑️ تم حذف الملف: 1358.json\n",
      "🗑️ تم حذف الملف: 1359.json\n",
      "🗑️ تم حذف الملف: 136.json\n",
      "🗑️ تم حذف الملف: 1360.json\n",
      "🗑️ تم حذف الملف: 1361.json\n",
      "🗑️ تم حذف الملف: 1362.json\n",
      "🗑️ تم حذف الملف: 1363.json\n",
      "🗑️ تم حذف الملف: 1364.json\n",
      "🗑️ تم حذف الملف: 1365.json\n",
      "🗑️ تم حذف الملف: 1366.json\n",
      "🗑️ تم حذف الملف: 1368.json\n",
      "🗑️ تم حذف الملف: 1369.json\n",
      "🗑️ تم حذف الملف: 137.json\n",
      "🗑️ تم حذف الملف: 1370.json\n",
      "🗑️ تم حذف الملف: 1371.json\n",
      "🗑️ تم حذف الملف: 1372.json\n",
      "🗑️ تم حذف الملف: 1373.json\n",
      "🗑️ تم حذف الملف: 1374.json\n",
      "🗑️ تم حذف الملف: 1375.json\n",
      "🗑️ تم حذف الملف: 1376.json\n",
      "🗑️ تم حذف الملف: 1377.json\n",
      "🗑️ تم حذف الملف: 1378.json\n",
      "🗑️ تم حذف الملف: 1379.json\n",
      "🗑️ تم حذف الملف: 138.json\n",
      "🗑️ تم حذف الملف: 1380.json\n",
      "🗑️ تم حذف الملف: 1381.json\n",
      "🗑️ تم حذف الملف: 1382.json\n",
      "🗑️ تم حذف الملف: 1383.json\n",
      "🗑️ تم حذف الملف: 1384.json\n",
      "🗑️ تم حذف الملف: 1385.json\n",
      "🗑️ تم حذف الملف: 1386.json\n",
      "🗑️ تم حذف الملف: 1387.json\n",
      "🗑️ تم حذف الملف: 1388.json\n",
      "🗑️ تم حذف الملف: 1389.json\n",
      "🗑️ تم حذف الملف: 139.json\n",
      "🗑️ تم حذف الملف: 1390.json\n",
      "🗑️ تم حذف الملف: 1391.json\n",
      "🗑️ تم حذف الملف: 1392.json\n",
      "🗑️ تم حذف الملف: 1393.json\n",
      "🗑️ تم حذف الملف: 1394.json\n",
      "🗑️ تم حذف الملف: 1395.json\n",
      "🗑️ تم حذف الملف: 1396.json\n",
      "🗑️ تم حذف الملف: 1397.json\n",
      "🗑️ تم حذف الملف: 1398.json\n",
      "🗑️ تم حذف الملف: 1399.json\n",
      "🗑️ تم حذف الملف: 14.json\n",
      "🗑️ تم حذف الملف: 140.json\n",
      "🗑️ تم حذف الملف: 1400.json\n",
      "🗑️ تم حذف الملف: 1402.json\n",
      "🗑️ تم حذف الملف: 1403.json\n",
      "🗑️ تم حذف الملف: 1405.json\n",
      "🗑️ تم حذف الملف: 1406.json\n",
      "🗑️ تم حذف الملف: 1407.json\n",
      "🗑️ تم حذف الملف: 1408.json\n",
      "🗑️ تم حذف الملف: 1409.json\n",
      "🗑️ تم حذف الملف: 141.json\n",
      "🗑️ تم حذف الملف: 1411.json\n",
      "🗑️ تم حذف الملف: 1415.json\n",
      "🗑️ تم حذف الملف: 1416.json\n",
      "🗑️ تم حذف الملف: 1417.json\n",
      "🗑️ تم حذف الملف: 1419.json\n",
      "🗑️ تم حذف الملف: 142.json\n",
      "🗑️ تم حذف الملف: 1420.json\n",
      "🗑️ تم حذف الملف: 1421.json\n",
      "🗑️ تم حذف الملف: 1424.json\n",
      "🗑️ تم حذف الملف: 1425.json\n",
      "🗑️ تم حذف الملف: 1426.json\n",
      "🗑️ تم حذف الملف: 1427.json\n",
      "🗑️ تم حذف الملف: 1428.json\n",
      "🗑️ تم حذف الملف: 143.json\n",
      "🗑️ تم حذف الملف: 1431.json\n",
      "🗑️ تم حذف الملف: 1433.json\n",
      "🗑️ تم حذف الملف: 1434.json\n",
      "🗑️ تم حذف الملف: 1435.json\n",
      "🗑️ تم حذف الملف: 1436.json\n",
      "🗑️ تم حذف الملف: 1437.json\n",
      "🗑️ تم حذف الملف: 1438.json\n",
      "🗑️ تم حذف الملف: 1439.json\n",
      "🗑️ تم حذف الملف: 144.json\n",
      "🗑️ تم حذف الملف: 1440.json\n",
      "🗑️ تم حذف الملف: 1441.json\n",
      "🗑️ تم حذف الملف: 1442.json\n",
      "🗑️ تم حذف الملف: 1443.json\n",
      "🗑️ تم حذف الملف: 1444.json\n",
      "🗑️ تم حذف الملف: 1445.json\n",
      "🗑️ تم حذف الملف: 1449.json\n",
      "🗑️ تم حذف الملف: 145.json\n",
      "🗑️ تم حذف الملف: 1450.json\n",
      "🗑️ تم حذف الملف: 1451.json\n",
      "🗑️ تم حذف الملف: 1452.json\n",
      "🗑️ تم حذف الملف: 1453.json\n",
      "🗑️ تم حذف الملف: 1454.json\n",
      "🗑️ تم حذف الملف: 1456.json\n",
      "🗑️ تم حذف الملف: 1457.json\n",
      "🗑️ تم حذف الملف: 1458.json\n",
      "🗑️ تم حذف الملف: 1459.json\n",
      "🗑️ تم حذف الملف: 146.json\n",
      "🗑️ تم حذف الملف: 1460.json\n",
      "🗑️ تم حذف الملف: 1461.json\n",
      "🗑️ تم حذف الملف: 1462.json\n",
      "🗑️ تم حذف الملف: 1463.json\n",
      "🗑️ تم حذف الملف: 1464.json\n",
      "🗑️ تم حذف الملف: 1467.json\n",
      "🗑️ تم حذف الملف: 1468.json\n",
      "🗑️ تم حذف الملف: 1469.json\n",
      "🗑️ تم حذف الملف: 147.json\n",
      "🗑️ تم حذف الملف: 1470.json\n",
      "🗑️ تم حذف الملف: 1471.json\n",
      "🗑️ تم حذف الملف: 1472.json\n",
      "🗑️ تم حذف الملف: 1473.json\n",
      "🗑️ تم حذف الملف: 1474.json\n",
      "🗑️ تم حذف الملف: 1475.json\n",
      "🗑️ تم حذف الملف: 1476.json\n",
      "🗑️ تم حذف الملف: 1477.json\n",
      "🗑️ تم حذف الملف: 1478.json\n",
      "🗑️ تم حذف الملف: 1479.json\n",
      "🗑️ تم حذف الملف: 148.json\n",
      "🗑️ تم حذف الملف: 1480.json\n",
      "🗑️ تم حذف الملف: 1482.json\n",
      "🗑️ تم حذف الملف: 1483.json\n",
      "🗑️ تم حذف الملف: 1484.json\n",
      "🗑️ تم حذف الملف: 1485.json\n",
      "🗑️ تم حذف الملف: 1486.json\n",
      "🗑️ تم حذف الملف: 1487.json\n",
      "🗑️ تم حذف الملف: 1488.json\n",
      "🗑️ تم حذف الملف: 1489.json\n",
      "🗑️ تم حذف الملف: 149.json\n",
      "🗑️ تم حذف الملف: 1490.json\n",
      "🗑️ تم حذف الملف: 1491.json\n",
      "🗑️ تم حذف الملف: 1492.json\n",
      "🗑️ تم حذف الملف: 1493.json\n",
      "🗑️ تم حذف الملف: 1495.json\n",
      "🗑️ تم حذف الملف: 1496.json\n",
      "🗑️ تم حذف الملف: 1497.json\n",
      "🗑️ تم حذف الملف: 1498.json\n",
      "🗑️ تم حذف الملف: 1499.json\n",
      "🗑️ تم حذف الملف: 15.json\n",
      "🗑️ تم حذف الملف: 150.json\n",
      "🗑️ تم حذف الملف: 1500.json\n",
      "🗑️ تم حذف الملف: 1502.json\n",
      "🗑️ تم حذف الملف: 1504.json\n",
      "🗑️ تم حذف الملف: 1505.json\n",
      "🗑️ تم حذف الملف: 1506.json\n",
      "🗑️ تم حذف الملف: 1507.json\n",
      "🗑️ تم حذف الملف: 1508.json\n",
      "🗑️ تم حذف الملف: 1509.json\n",
      "🗑️ تم حذف الملف: 151.json\n",
      "🗑️ تم حذف الملف: 1510.json\n",
      "🗑️ تم حذف الملف: 1511.json\n",
      "🗑️ تم حذف الملف: 1512.json\n",
      "🗑️ تم حذف الملف: 1513.json\n",
      "🗑️ تم حذف الملف: 1519.json\n",
      "🗑️ تم حذف الملف: 152.json\n",
      "🗑️ تم حذف الملف: 1521.json\n",
      "🗑️ تم حذف الملف: 1524.json\n",
      "🗑️ تم حذف الملف: 1525.json\n",
      "🗑️ تم حذف الملف: 1526.json\n",
      "🗑️ تم حذف الملف: 1527.json\n",
      "🗑️ تم حذف الملف: 1528.json\n",
      "🗑️ تم حذف الملف: 1529.json\n",
      "🗑️ تم حذف الملف: 153.json\n",
      "🗑️ تم حذف الملف: 1530.json\n",
      "🗑️ تم حذف الملف: 1531.json\n",
      "🗑️ تم حذف الملف: 1532.json\n",
      "🗑️ تم حذف الملف: 1533.json\n",
      "🗑️ تم حذف الملف: 1534.json\n",
      "🗑️ تم حذف الملف: 1536.json\n",
      "🗑️ تم حذف الملف: 1538.json\n",
      "🗑️ تم حذف الملف: 1539.json\n",
      "🗑️ تم حذف الملف: 154.json\n",
      "🗑️ تم حذف الملف: 1540.json\n",
      "🗑️ تم حذف الملف: 1541.json\n",
      "🗑️ تم حذف الملف: 1542.json\n",
      "🗑️ تم حذف الملف: 1543.json\n",
      "🗑️ تم حذف الملف: 1544.json\n",
      "🗑️ تم حذف الملف: 1545.json\n",
      "🗑️ تم حذف الملف: 1546.json\n",
      "🗑️ تم حذف الملف: 1547.json\n",
      "🗑️ تم حذف الملف: 1548.json\n",
      "🗑️ تم حذف الملف: 1549.json\n",
      "🗑️ تم حذف الملف: 155.json\n",
      "🗑️ تم حذف الملف: 1550.json\n",
      "🗑️ تم حذف الملف: 1551.json\n",
      "🗑️ تم حذف الملف: 1552.json\n",
      "🗑️ تم حذف الملف: 1553.json\n",
      "🗑️ تم حذف الملف: 1554.json\n",
      "🗑️ تم حذف الملف: 1555.json\n",
      "🗑️ تم حذف الملف: 1556.json\n",
      "🗑️ تم حذف الملف: 1557.json\n",
      "🗑️ تم حذف الملف: 1558.json\n",
      "🗑️ تم حذف الملف: 1559.json\n",
      "🗑️ تم حذف الملف: 156.json\n",
      "🗑️ تم حذف الملف: 1560.json\n",
      "🗑️ تم حذف الملف: 1563.json\n",
      "🗑️ تم حذف الملف: 1565.json\n",
      "🗑️ تم حذف الملف: 1566.json\n",
      "🗑️ تم حذف الملف: 1567.json\n",
      "🗑️ تم حذف الملف: 1568.json\n",
      "🗑️ تم حذف الملف: 1569.json\n",
      "🗑️ تم حذف الملف: 157.json\n",
      "🗑️ تم حذف الملف: 1570.json\n",
      "🗑️ تم حذف الملف: 1571.json\n",
      "🗑️ تم حذف الملف: 1572.json\n",
      "🗑️ تم حذف الملف: 1573.json\n",
      "🗑️ تم حذف الملف: 1574.json\n",
      "🗑️ تم حذف الملف: 1575.json\n",
      "🗑️ تم حذف الملف: 1576.json\n",
      "🗑️ تم حذف الملف: 1577.json\n",
      "🗑️ تم حذف الملف: 158.json\n",
      "🗑️ تم حذف الملف: 1584.json\n",
      "🗑️ تم حذف الملف: 1586.json\n",
      "🗑️ تم حذف الملف: 1587.json\n",
      "🗑️ تم حذف الملف: 1588.json\n",
      "🗑️ تم حذف الملف: 1589.json\n",
      "🗑️ تم حذف الملف: 159.json\n",
      "🗑️ تم حذف الملف: 1593.json\n",
      "🗑️ تم حذف الملف: 1594.json\n",
      "🗑️ تم حذف الملف: 1595.json\n",
      "🗑️ تم حذف الملف: 1596.json\n",
      "🗑️ تم حذف الملف: 1599.json\n",
      "🗑️ تم حذف الملف: 16.json\n",
      "🗑️ تم حذف الملف: 160.json\n",
      "🗑️ تم حذف الملف: 1601.json\n",
      "🗑️ تم حذف الملف: 1603.json\n",
      "🗑️ تم حذف الملف: 1604.json\n",
      "🗑️ تم حذف الملف: 1606.json\n",
      "🗑️ تم حذف الملف: 1608.json\n",
      "🗑️ تم حذف الملف: 1609.json\n",
      "🗑️ تم حذف الملف: 161.json\n",
      "🗑️ تم حذف الملف: 1610.json\n",
      "🗑️ تم حذف الملف: 1612.json\n",
      "🗑️ تم حذف الملف: 1613.json\n",
      "🗑️ تم حذف الملف: 1614.json\n",
      "🗑️ تم حذف الملف: 1615.json\n",
      "🗑️ تم حذف الملف: 1616.json\n",
      "🗑️ تم حذف الملف: 1618.json\n",
      "🗑️ تم حذف الملف: 1619.json\n",
      "🗑️ تم حذف الملف: 162.json\n",
      "🗑️ تم حذف الملف: 1620.json\n",
      "🗑️ تم حذف الملف: 1621.json\n",
      "🗑️ تم حذف الملف: 1622.json\n",
      "🗑️ تم حذف الملف: 1625.json\n",
      "🗑️ تم حذف الملف: 1626.json\n",
      "🗑️ تم حذف الملف: 1627.json\n",
      "🗑️ تم حذف الملف: 1628.json\n",
      "🗑️ تم حذف الملف: 1629.json\n",
      "🗑️ تم حذف الملف: 163.json\n",
      "🗑️ تم حذف الملف: 1632.json\n",
      "🗑️ تم حذف الملف: 1636.json\n",
      "🗑️ تم حذف الملف: 1637.json\n",
      "🗑️ تم حذف الملف: 1639.json\n",
      "🗑️ تم حذف الملف: 164.json\n",
      "🗑️ تم حذف الملف: 1640.json\n",
      "🗑️ تم حذف الملف: 1642.json\n",
      "🗑️ تم حذف الملف: 1644.json\n",
      "🗑️ تم حذف الملف: 1647.json\n",
      "🗑️ تم حذف الملف: 1649.json\n",
      "🗑️ تم حذف الملف: 165.json\n",
      "🗑️ تم حذف الملف: 1652.json\n",
      "🗑️ تم حذف الملف: 1653.json\n",
      "🗑️ تم حذف الملف: 1654.json\n",
      "🗑️ تم حذف الملف: 1656.json\n",
      "🗑️ تم حذف الملف: 1659.json\n",
      "🗑️ تم حذف الملف: 166.json\n",
      "🗑️ تم حذف الملف: 1660.json\n",
      "🗑️ تم حذف الملف: 1661.json\n",
      "🗑️ تم حذف الملف: 1664.json\n",
      "🗑️ تم حذف الملف: 1665.json\n",
      "🗑️ تم حذف الملف: 1666.json\n",
      "🗑️ تم حذف الملف: 1667.json\n",
      "🗑️ تم حذف الملف: 1668.json\n",
      "🗑️ تم حذف الملف: 1669.json\n",
      "🗑️ تم حذف الملف: 167.json\n",
      "🗑️ تم حذف الملف: 1671.json\n",
      "🗑️ تم حذف الملف: 1672.json\n",
      "🗑️ تم حذف الملف: 1673.json\n",
      "🗑️ تم حذف الملف: 1674.json\n",
      "🗑️ تم حذف الملف: 1675.json\n",
      "🗑️ تم حذف الملف: 1676.json\n",
      "🗑️ تم حذف الملف: 1678.json\n",
      "🗑️ تم حذف الملف: 1679.json\n",
      "🗑️ تم حذف الملف: 168.json\n",
      "🗑️ تم حذف الملف: 1682.json\n",
      "🗑️ تم حذف الملف: 169.json\n",
      "✅ تم حذف جميع الملفات المطابقة من مجلد Sahih_muslim.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# مجلد ملفات JSON الأصلية\n",
    "processed_folder = r\"D:\\زينب\\Sahih_muslim_processed\"\n",
    "\n",
    "# مجلد صحيح مسلم الذي سيتم حذف الملفات منه\n",
    "original_folder = r\"D:\\زينب\\Sahih_muslim\"\n",
    "\n",
    "# الحصول على أسماء ملفات JSON من مجلد المعالجة (بدون الامتداد)\n",
    "json_names = [os.path.splitext(f)[0] for f in os.listdir(processed_folder) if f.endswith(\".json\")]\n",
    "\n",
    "# التكرار على الملفات داخل مجلد صحيح مسلم\n",
    "for filename in os.listdir(original_folder):\n",
    "    file_path = os.path.join(original_folder, filename)\n",
    "\n",
    "    # التحقق إن كان هذا ملف JSON واسمه موجود في قائمة الأسماء\n",
    "    if filename.endswith(\".json\"):\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        if name_without_ext in json_names:\n",
    "            os.remove(file_path)\n",
    "            print(f\"🗑️ تم حذف الملف: {filename}\")\n",
    "\n",
    "print(\"✅ تم حذف جميع الملفات المطابقة من مجلد Sahih_muslim.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
