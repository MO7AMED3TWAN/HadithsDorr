{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Processing Hadith Dataset\n",
    "\n",
    "1. **Define Validation Models and Utilities**\n",
    "   - Define a Pydantic model (`QuestionAnswer`) for validating question-answer pairs.\n",
    "   - Implement a function (`estimate_tokens`) to estimate the number of tokens in Arabic text.\n",
    "\n",
    "2. **Prepare the Prompt Template**\n",
    "   - Create a prompt template (`PROMPT_TEMPLATE`) that instructs the language model to answer four fixed questions about each hadith, using Modern Standard Arabic with full diacritics.\n",
    "\n",
    "3. **Process a Single Hadith Dataset File**\n",
    "   - Load the input JSON file containing hadith entries.\n",
    "   - For each entry:\n",
    "     - Skip entries without a valid explanation (`sharh`).\n",
    "     - Format the prompt with the hadith and its explanation.\n",
    "     - Estimate the number of tokens in the prompt.\n",
    "     - Send the prompt to the Together API and stream the response.\n",
    "     - Collect and clean the model's output, ensuring it is a JSON array of four answers.\n",
    "     - Map each answer to its corresponding question and update the entry with:\n",
    "       - `FT_Pairs`: List of question-answer pairs.\n",
    "       - `hadith_lessons`: First answer as a lesson.\n",
    "       - `hadith_application`: Second answer as an application.\n",
    "     - Handle errors by setting empty values if processing fails.\n",
    "   - Save the enriched data to the output JSON file.\n",
    "   - Print statistics about the processing.\n",
    "\n",
    "4. **Process All Hadith Files in a Directory**\n",
    "   - Create the output directory if it does not exist.\n",
    "   - Find all JSON files in the input directory.\n",
    "   - For each file:\n",
    "     - Process the file using the above function.\n",
    "     - Aggregate statistics across all files.\n",
    "     - Add a delay between files to avoid rate limiting.\n",
    "   - Print an overall summary of the processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Together' from 'together' (/home/mohamed/.local/lib/python3.12/site-packages/together/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtogether\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Together\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Together' from 'together' (/home/mohamed/.local/lib/python3.12/site-packages/together/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from together import Together\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Together client\n",
    "client = Together(api_key=\"tgp_v1_nG8tM-osJ_jwKSPsPiRZnr6IiartvAs5A8IexEAoyxk\")\n",
    "\n",
    "# Define fixed questions with diacritics\n",
    "def get_fixed_questions() -> List[Dict[str, str]]:\n",
    "    \"\"\"Return the fixed questions with their full diacritics\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"question\": \"مَا هِيَ الرَّسَائِلُ الرَّئِيسِيَّةُ وَالدُّرُوسُ المُسْتَفَادَةُ والفَوَائِد المُستَخلصَة مِنَ الحَدِيثِ؟\",\n",
    "            \"answer\": \"{answer1}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"كَيْفَ يُمْكِنُ تَطْبِيقُ الحَدِيثِ فِي الحَيَاةِ اليَوْمِيَّةِ؟\",\n",
    "            \"answer\": \"{answer2}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"مَا أَهَمِّيَّةُ الحَدِيثِ فِي الفِقْهِ الإِسْلَامِيِّ؟\",\n",
    "            \"answer\": \"{answer3}\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"مَا سَبَبُ وُرُودِ الحَدِيثِ؟\",\n",
    "            \"answer\": \"{answer4}\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for validation\n",
    "class QuestionAnswer(BaseModel):\n",
    "    \"\"\"Model for question-answer pairs\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# Function to estimate token count (rough approximation)\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count in a string (rough approximation)\"\"\"\n",
    "    # For Arabic text, a rough estimation is about 1 token per 2.5 characters\n",
    "    return len(text) // 2\n",
    "\n",
    "# Create a simplified prompt template that only asks for answers, not questions\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert in analyzing Prophetic Hadiths and Islamic jurisprudence.\n",
    "Your task is to analyze an Arabic Hadith using the provided explanation to extract knowledge, jurisprudential rulings, and practical applications.\n",
    "\n",
    "== INPUT ==\n",
    "You will receive:\n",
    "- 'hadith': Text of the Prophetic Hadith in Arabic (with diacritics).\n",
    "- 'explanation': Detailed explanation of the Hadith (Arabic text).\n",
    "\n",
    "== TASK ==\n",
    "Based on the provided Hadith, explanation, and your knowledge, answer the following questions in JSON format.\n",
    "\n",
    "== FIXED QUESTIONS ==\n",
    "Answer ONLY these questions without repeating the question text:\n",
    "1. What are the main messages, lessons learned, and benefits derived from the Hadith?\n",
    "2. How can the Hadith be applied in daily life?\n",
    "3. What is the importance of the Hadith in Islamic jurisprudence?\n",
    "4. What is the reason or context behind the narration of the Hadith?\n",
    "\n",
    "== RULES ==\n",
    "- Use Modern Standard Arabic with full diacritics in your answers.\n",
    "- Base your answers on the provided explanation and your knowledge of authentic Hadiths.\n",
    "- Ensure each answer accurately reflects the content and meaning of the Hadith without incorporating personal interpretations or conclusions.\n",
    "- Verify the authenticity of all information before preparing your response.\n",
    "- Return ONLY a JSON array with your answers as shown below, without any additional comments or explanations.\n",
    "\n",
    "== Expected Input ==\n",
    "{{\n",
    "  \"hadith\": \"{hadith}\",\n",
    "  \"sharh\": \"{sharh}\"\n",
    "}}\n",
    "\n",
    "== Expected Output (JSON Array) ==\n",
    "[\n",
    "  \"الإجابة الأولى مع التشكيل الكامل...\",\n",
    "  \"الإجابة الثانية مع التشكيل الكامل...\",\n",
    "  \"الإجابة الثالثة مع التشكيل الكامل...\",\n",
    "  \"الإجابة الرابعة مع التشكيل الكامل...\"\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "def process_hadith_dataset(input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Process the hadith dataset, enriching it with QA pairs\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input JSON file\n",
    "        output_file: Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    # Load hadith dataset\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get fixed questions template\n",
    "    fixed_questions = get_fixed_questions()\n",
    "    \n",
    "    # Process each hadith with a progress bar\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(f\"Processing {len(data)} hadith entries from {input_file}...\")\n",
    "    for entry in tqdm(data, desc=f\"Processing hadiths from {os.path.basename(input_file)}\"):\n",
    "        # Skip if no sharh is available\n",
    "        if not entry.get(\"sharh\") or entry[\"sharh\"] == \".\":\n",
    "            print(f\"Skipping hadith ID {entry.get('hadith_id', 'unknown')} - no sharh available\")\n",
    "            continue\n",
    "        \n",
    "        # Create input for LLM - use the same file without creating a new one\n",
    "        prompt = PROMPT_TEMPLATE.format(\n",
    "            hadith=entry[\"hadith\"],\n",
    "            sharh=entry[\"sharh\"]\n",
    "        )\n",
    "        \n",
    "        # Estimate tokens before generation\n",
    "        tokens_before = estimate_tokens(prompt)\n",
    "        total_tokens_before += tokens_before\n",
    "        \n",
    "        # Stream response from Together\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-V3\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Collect response\n",
    "            output_text = \"\"\n",
    "            for token in response:\n",
    "                if hasattr(token, 'choices') and token.choices[0].delta.content:\n",
    "                    output_text += token.choices[0].delta.content\n",
    "            \n",
    "            # Estimate tokens after generation\n",
    "            tokens_after = estimate_tokens(output_text)\n",
    "            total_tokens_after += tokens_after\n",
    "            \n",
    "            # Process the output - directly match keys and values\n",
    "            try:\n",
    "                # Clean the output text - handle potential formatting issues\n",
    "                output_text = output_text.strip()\n",
    "                if output_text.startswith(\"```json\"):\n",
    "                    output_text = output_text[7:]\n",
    "                if output_text.endswith(\"```\"):\n",
    "                    output_text = output_text[:-3]\n",
    "                output_text = output_text.strip()\n",
    "                \n",
    "                # Parse as regular JSON - expecting an array of 4 strings\n",
    "                answer_list = json.loads(output_text)\n",
    "                \n",
    "                if not isinstance(answer_list, list):\n",
    "                    raise ValueError(f\"Expected list of answers, got: {type(answer_list)}\")\n",
    "                \n",
    "                # Ensure we have at least 4 answers, pad with empty strings if needed\n",
    "                while len(answer_list) < 4:\n",
    "                    answer_list.append(\"\")\n",
    "                \n",
    "                # Use direct key matching instead of creating new objects\n",
    "                qa_pairs = []\n",
    "                for i, answer in enumerate(answer_list[:4]):  # Limit to first 4 answers\n",
    "                    qa_pair = {\n",
    "                        \"question\": fixed_questions[i][\"question\"],\n",
    "                        \"answer\": answer\n",
    "                    }\n",
    "                    qa_pairs.append(qa_pair)\n",
    "                \n",
    "                # Update the existing entry directly\n",
    "                entry[\"FT_Pairs\"] = qa_pairs\n",
    "                \n",
    "                # Extract first two answers directly into lessons and application\n",
    "                entry[\"hadith_lessons\"] = [answer_list[0]] if answer_list[0] else []\n",
    "                entry[\"hadith_application\"] = [answer_list[1]] if answer_list[1] else []\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"\\n[!] Processing error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "                # Set empty values for failed processing\n",
    "                entry[\"FT_Pairs\"] = []\n",
    "                entry[\"hadith_lessons\"] = []\n",
    "                entry[\"hadith_application\"] = []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] API error for hadith ID {entry.get('hadith_id', 'unknown')}: {str(e)}\")\n",
    "            entry[\"FT_Pairs\"] = []\n",
    "            entry[\"hadith_lessons\"] = []\n",
    "            entry[\"hadith_application\"] = []\n",
    "    \n",
    "    # Save the enriched data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nProcessing completed for file {input_file}:\")\n",
    "    print(f\"- Successfully processed: {processed_count}/{len(data)} entries\")\n",
    "    print(f\"- Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"- Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"- Token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"- Dataset saved to '{output_file}'\")\n",
    "    \n",
    "    return processed_count, total_tokens_before, total_tokens_after\n",
    "\n",
    "def process_all_hadith_files(input_dir: str, output_dir: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Process all hadith JSON files in a directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to the directory containing input JSON files\n",
    "        output_dir: Path to save the output JSON files (if None, will use input_dir with '_processed' suffix)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if output_dir is None:\n",
    "        output_dir = input_dir + \"_processed\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files in the input directory\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    total_processed = 0\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    \n",
    "    for input_file in json_files:\n",
    "        file_name = os.path.basename(input_file)\n",
    "        output_file = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        print(f\"\\nProcessing file: {file_name}\")\n",
    "        processed, tokens_before, tokens_after = process_hadith_dataset(input_file, output_file)\n",
    "        \n",
    "        total_processed += processed\n",
    "        total_tokens_before += tokens_before\n",
    "        total_tokens_after += tokens_after\n",
    "        \n",
    "        # Add a small delay between files to avoid rate limiting\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"\\n===== OVERALL PROCESSING SUMMARY =====\")\n",
    "    print(f\"Total files processed: {len(json_files)}\")\n",
    "    print(f\"Total hadiths processed: {total_processed}\")\n",
    "    print(f\"Total tokens before generation: {total_tokens_before}\")\n",
    "    print(f\"Total tokens after generation: {total_tokens_after}\")\n",
    "    if total_tokens_after > 0:\n",
    "        print(f\"Overall token reduction ratio: {total_tokens_before/total_tokens_after:.2f}x\")\n",
    "    print(f\"All processed files saved to '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all JSON files in the Sahih_muslim directory\n",
    "    input_dir = r\"d:\\زينب\\Sahih_muslim\"\n",
    "    output_dir = r\"d:\\زينب\\Sahih_muslim_processed\"\n",
    "    \n",
    "    process_all_hadith_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the mattan's hadith to questions answers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # مكتبة شريط التقدم\n",
    "\n",
    "# مسار المجلد الذي يحتوي على ملفات JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# الحصول على قائمة ملفات JSON\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "\n",
    "# التكرار مع tqdm لعرض شريط التقدم\n",
    "for filename in tqdm(json_files, desc=\"معالجة الملفات\"):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # فتح ملف البيانات للقراءة\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # معالجة كل عنصر في البيانات\n",
    "    for entry in data:\n",
    "        hadith_text = entry.get(\"hadith\", \"\")\n",
    "\n",
    "        # تحديث الأسئلة في FT_Pairs\n",
    "        for pair in entry.get(\"FT_Pairs\", []):\n",
    "            if \"question\" in pair:\n",
    "                pair[\"question\"] = pair[\"question\"].replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "\n",
    "        # تحديث hadith_lessons و hadith_application إذا كانت موجودة\n",
    "        if \"hadith_lessons\" in entry and entry[\"hadith_lessons\"]:\n",
    "            for i, lesson in enumerate(entry[\"hadith_lessons\"]):\n",
    "                entry[\"hadith_lessons\"][i] = lesson.replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "        \n",
    "        if \"hadith_application\" in entry and entry[\"hadith_application\"]:\n",
    "            for i, app in enumerate(entry[\"hadith_application\"]):\n",
    "                entry[\"hadith_application\"][i] = app.replace(\"الحَدِيثِ\", f\"الحَدِيثِ'{hadith_text}'\")\n",
    "\n",
    "    # حفظ البيانات المعدلة\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ تم تحديث جميع ملفات الحديث بنجاح!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Empty JSON Files to Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# مجلد ملفات JSON\n",
    "folder_path = \"Sahih_muslim_processed\"\n",
    "\n",
    "# التكرار على جميع ملفات JSON في المجلد\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # فتح وتحميل الملف\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # التأكد أن الملف يحتوي على قائمة من العناصر\n",
    "            if isinstance(data, list):\n",
    "                all_empty = True\n",
    "                for entry in data:\n",
    "                    # فحص الحقول الثلاثة\n",
    "                    if (entry.get(\"hadith_lessons\") or \n",
    "                        entry.get(\"hadith_application\") or \n",
    "                        entry.get(\"FT_Pairs\")):\n",
    "                        all_empty = False\n",
    "                        break\n",
    "\n",
    "                # حذف الملف إذا كانت كل الحقول في كل العناصر فارغة\n",
    "                if all_empty:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"🗑️ تم حذف الملف لأن جميع الحقول فارغة: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ خطأ في قراءة الملف {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Processed JSON Files from Original Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# مجلد ملفات JSON الأصلية\n",
    "processed_folder = r\"D:\\زينب\\Sahih_muslim_processed\"\n",
    "\n",
    "# مجلد صحيح مسلم الذي سيتم حذف الملفات منه\n",
    "original_folder = r\"D:\\زينب\\Sahih_muslim\"\n",
    "\n",
    "# الحصول على أسماء ملفات JSON من مجلد المعالجة (بدون الامتداد)\n",
    "json_names = [os.path.splitext(f)[0] for f in os.listdir(processed_folder) if f.endswith(\".json\")]\n",
    "\n",
    "# التكرار على الملفات داخل مجلد صحيح مسلم\n",
    "for filename in os.listdir(original_folder):\n",
    "    file_path = os.path.join(original_folder, filename)\n",
    "\n",
    "    # التحقق إن كان هذا ملف JSON واسمه موجود في قائمة الأسماء\n",
    "    if filename.endswith(\".json\"):\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        if name_without_ext in json_names:\n",
    "            os.remove(file_path)\n",
    "            print(f\"🗑️ تم حذف الملف: {filename}\")\n",
    "\n",
    "print(\"✅ تم حذف جميع الملفات المطابقة من مجلد Sahih_muslim.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
