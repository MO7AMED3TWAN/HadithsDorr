{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def restructure_json(input_path):\n",
        "    # Read JSON file\n",
        "    with open(input_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    \n",
        "    # Restructure each hadith entry\n",
        "    restructured_data = []\n",
        "    for hadith in data:\n",
        "        restructured_hadith = {\n",
        "            \"hadith_id\": int(hadith[\"hadith_id\"]),  # Convert to int\n",
        "            \"hadith\": hadith[\"hadith\"],\n",
        "            \"rawi\": hadith[\"rawi\"],\n",
        "            \"mohdith\": hadith[\"mohdith\"],\n",
        "            \"book\": hadith[\"book\"],\n",
        "            \"hadith_number\": int(hadith[\"numberOrPage\"]),  # Rename and convert to int\n",
        "            \"grade\": hadith[\"grade\"],\n",
        "            \"sharh\": hadith[\"sharh\"],\n",
        "            # Convert lists to strings by taking first element\n",
        "            \"hadith_lessons\": hadith[\"hadith_lessons\"][0] if hadith[\"hadith_lessons\"] else \"\",\n",
        "            \"hadith_application\": hadith[\"hadith_application\"][0] if hadith[\"hadith_application\"] else \"\",\n",
        "            \"FT_Pairs\": hadith[\"FT_Pairs\"]\n",
        "        }\n",
        "        restructured_data.append(restructured_hadith)\n",
        "    \n",
        "    return restructured_data\n",
        "\n",
        "def process_all_folders(base_path):\n",
        "    # Create output directory\n",
        "    output_base = Path(base_path) / \"restructured_data\"\n",
        "    output_base.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Get all folders that might contain JSON files\n",
        "    folders = [f for f in Path(base_path).iterdir() if f.is_dir() and not f.name.startswith('.')]\n",
        "    \n",
        "    total_files = 0\n",
        "    # Count total JSON files first\n",
        "    for folder in folders:\n",
        "        json_files = list(folder.glob('*.json'))\n",
        "        total_files += len(json_files)\n",
        "    \n",
        "    print(f\"Found {len(folders)} folders with {total_files} JSON files total\")\n",
        "    \n",
        "    # Process each folder\n",
        "    with tqdm(total=total_files, desc=\"Processing files\") as pbar:\n",
        "        for folder in folders:\n",
        "            if folder == output_base:  # Skip the output directory\n",
        "                continue\n",
        "                \n",
        "            # Create corresponding output folder\n",
        "            output_folder = output_base / folder.name\n",
        "            output_folder.mkdir(exist_ok=True)\n",
        "            \n",
        "            # Process each JSON file in the folder\n",
        "            for json_file in folder.glob('*.json'):\n",
        "                try:\n",
        "                    # Restructure the data\n",
        "                    restructured_data = restructure_json(json_file)\n",
        "                    \n",
        "                    # Save to new location\n",
        "                    output_file = output_folder / json_file.name\n",
        "                    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(restructured_data, f, ensure_ascii=False, indent=2)\n",
        "                    \n",
        "                    pbar.update(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError processing {json_file}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 folders with 7548 JSON files total\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18fd7ea928b74557ba6d55cb50b1b1c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing files:   0%|          | 0/7548 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Usage\n",
        "folder_path = \"../new/\"  # Update this path\n",
        "process_all_folders(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_json_files(folder_path):\n",
        "    \"\"\"\n",
        "    Merges all JSON files in a folder using hadith_id as the key\n",
        "    \"\"\"\n",
        "    merged_data = []  # نستخدم قائمة بدلاً من dictionary\n",
        "    json_files = list(Path(folder_path).glob('*.json'))\n",
        "    \n",
        "    for json_file in tqdm(json_files, desc=f\"Processing {Path(folder_path).name}\"):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list):\n",
        "                    merged_data.extend(data)\n",
        "                else:\n",
        "                    merged_data.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {str(e)}\")\n",
        "    \n",
        "    # نقوم بترتيب البيانات حسب hadith_id\n",
        "    merged_data.sort(key=lambda x: x['hadith_id'])\n",
        "    \n",
        "    # نحول القائمة إلى dictionary\n",
        "    final_data = {}\n",
        "    for hadith in merged_data:\n",
        "        hadith_id = str(hadith.pop('hadith_id'))  # نحول الـ ID إلى string\n",
        "        final_data[f\"hadith_{hadith_id}\"] = hadith\n",
        "    \n",
        "    return [final_data]  # نرجع القاموس داخل قائمة\n",
        "\n",
        "def process_all_folders(base_path):\n",
        "    \"\"\"\n",
        "    Process all folders and create a single JSON file for each\n",
        "    \"\"\"\n",
        "    folders = [f for f in Path(base_path).iterdir() if f.is_dir() and not f.name.startswith('.')]\n",
        "    print(f\"Found {len(folders)} folders\")\n",
        "    \n",
        "    for folder in folders:\n",
        "        try:\n",
        "            merged_data = merge_json_files(folder)\n",
        "            output_file = folder.parent / f\"{folder.name}_merged.json\"\n",
        "            \n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(merged_data[0], f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            print(f\"✅ Created {output_file} with {len(merged_data[0])} entries\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing folder {folder}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 folders\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5884fdf047342558e201da5b850eabf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Sahih_muslim:   0%|          | 0/2239 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created /home/mohamed/Desktop/JUPYTER/Graduation/new/restructured_data/Sahih_muslim_merged.json with 2239 entries\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d223c869b1541ca943d24e3d302b6d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Sahih_bukhari:   0%|          | 0/5309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created /home/mohamed/Desktop/JUPYTER/Graduation/new/restructured_data/Sahih_bukhari_merged.json with 5309 entries\n"
          ]
        }
      ],
      "source": [
        "# Usage\n",
        "base_path = \"/home/mohamed/Desktop/JUPYTER/Graduation/new/restructured_data/\"\n",
        "process_all_folders(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of hadiths in the merged file: 2239\n"
          ]
        }
      ],
      "source": [
        "# Path to the merged JSON file\n",
        "merged_file_path = f\"./Sahih_muslim_merged.json\"\n",
        "\n",
        "# Load the merged JSON file and count the hadiths\n",
        "with open(merged_file_path, 'r', encoding='utf-8') as f:\n",
        "    merged_data = json.load(f)\n",
        "    hadith_count = len(merged_data)\n",
        "    print(f\"Total number of hadiths in the merged file: {hadith_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of hadiths in the merged file: 5309\n"
          ]
        }
      ],
      "source": [
        "merged_file_path = f\"./Sahih_bukhari_merged.json\"\n",
        "\n",
        "with open(merged_file_path, 'r', encoding='utf-8') as f:\n",
        "    merged_data = json.load(f)\n",
        "    hadith_count = len(merged_data)\n",
        "    print(f\"Total number of hadiths in the merged file: {hadith_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ7mogMPqJAu"
      },
      "source": [
        "# Data handling for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_merged_hadith_data(file_path):\n",
        "    \"\"\"Extracts 'hadith', 'rawi', 'book', and 'sharh' from a merged JSON file\"\"\"\n",
        "    extracted_data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            merged_data = json.load(f)\n",
        "            # Extract data from each hadith in the merged file\n",
        "            for hadith_key, hadith_value in tqdm(merged_data.items(), desc=\"Extracting hadiths\"):\n",
        "                extracted_data.append({\n",
        "                    'hadith': hadith_value.get('hadith'),\n",
        "                    'rawi': hadith_value.get('rawi'),\n",
        "                    'book': hadith_value.get('book'),\n",
        "                    'sharh': hadith_value.get('sharh'),\n",
        "                    'hadith_number': hadith_value.get('hadith_number'),\n",
        "                    'grade': hadith_value.get('grade'),\n",
        "                    # 'FT_Pairs': hadith_value.get('FT_Pairs')\n",
        "                })\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON from {file_path}: {str(e)}\")\n",
        "    return extracted_data\n",
        "\n",
        "def process_merged_files(input_files, output_file):\n",
        "    \"\"\"Process merged hadith files and save extracted data\"\"\"\n",
        "    all_hadith_data = []\n",
        "    \n",
        "    # Process each merged file\n",
        "    for file_path in input_files:\n",
        "        print(f\"Processing {Path(file_path).name}...\")\n",
        "        extracted_data = extract_merged_hadith_data(file_path)\n",
        "        all_hadith_data.extend(extracted_data)\n",
        "    \n",
        "    # Save the extracted data\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_hadith_data, f, ensure_ascii=False, indent=4)\n",
        "        \n",
        "    return len(all_hadith_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Sahih_bukhari_merged.json...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eef63f2e270d421c9b049f2e84b76b57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting hadiths:   0%|          | 0/5309 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Sahih_muslim_merged.json...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b98ceedc19c44a95973eaef65858c8cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting hadiths:   0%|          | 0/2239 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extracted data saved to ./extracted_hadith_data.json\n",
            "Total hadiths processed: 7548\n"
          ]
        }
      ],
      "source": [
        "# Input and output paths\n",
        "input_files = [\n",
        "    \"./Sahih_bukhari_merged.json\",\n",
        "    \"./Sahih_muslim_merged.json\"\n",
        "]\n",
        "output_file = './extracted_hadith_data.json'\n",
        "\n",
        "# Process the files\n",
        "total_hadiths = process_merged_files(input_files, output_file)\n",
        "print(f\"✅ Extracted data saved to {output_file}\")\n",
        "print(f\"Total hadiths processed: {total_hadiths}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تم تحويل 7548 حديثًا إلى ملف واحد (formatted_hadiths.json).\n"
          ]
        }
      ],
      "source": [
        "# 1. قراءة ملف JSON الأصلي\n",
        "with open('./extracted_hadith_data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)  # يجب أن يكون ملفك عبارة عن قائمة من الأوبجيكتس\n",
        "\n",
        "# 2. تحويل كل حديث إلى زوج (سؤال-إجابة)\n",
        "formatted_data = []\n",
        "for item in data:\n",
        "    question = f'ما هي تفاصيل الحديث \"{item[\"hadith\"]}\" وما هو الشرح الكافي له؟'\n",
        "    answer = (\n",
        "        f'ورد هذا الحديث فى كتاب {item[\"book\"]} برقم {item[\"hadith_number\"]} '\n",
        "        f'ورواه {item[\"rawi\"]} ودرجته {item[\"grade\"]}.\\n'\n",
        "        f'أما شرح الحديث: {item[\"sharh\"]}'\n",
        "    )\n",
        "    formatted_data.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "# 3. خلط البيانات عشوائيًا\n",
        "random.shuffle(formatted_data)\n",
        "\n",
        "# 4. حفظ البيانات في ملف JSON واحد\n",
        "with open('formatted_hadiths.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"تم تحويل {len(formatted_data)} حديثًا إلى ملف واحد (formatted_hadiths.json).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKnH5-ueqNWe"
      },
      "outputs": [],
      "source": [
        "# def extract_hadith_data(file_path):\n",
        "#     \"\"\"Extracts 'hadith', 'rawi', 'book', and 'sharh' from a JSON file.\"\"\"\n",
        "#     data = []\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         try:\n",
        "#             file_data = json.load(f)\n",
        "#             for item in file_data:\n",
        "#                 extracted_data = {\n",
        "#                     'hadith': item.get('hadith'),\n",
        "#                     'rawi': item.get('rawi'),\n",
        "#                     'book': item.get('book'),\n",
        "#                     'sharh': item.get('sharh')\n",
        "#                 }\n",
        "#                 data.append(extracted_data)\n",
        "#         except json.JSONDecodeError:\n",
        "#             print(f\"Error decoding JSON from {file_path}\")\n",
        "#             pass  # Skip invalid JSON files\n",
        "#     return data\n",
        "\n",
        "# def merge_hadith_files(bukhari_dir, muslim_dir, output_file):\n",
        "#     \"\"\"\n",
        "#     Iterates through JSON files in Bukhari and Muslim directories,\n",
        "#     extracts specified fields, and merges them into a single JSON file\n",
        "#     with a progress bar.\n",
        "#     \"\"\"\n",
        "#     all_hadith_data = []\n",
        "\n",
        "#     # Get list of files to process\n",
        "#     bukhari_files = [os.path.join(bukhari_dir, f) for f in os.listdir(bukhari_dir) if f.endswith('.json')]\n",
        "#     muslim_files = [os.path.join(muslim_dir, f) for f in os.listdir(muslim_dir) if f.endswith('.json')]\n",
        "#     all_files = bukhari_files + muslim_files\n",
        "\n",
        "#     # Process files with a progress bar\n",
        "#     for file_path in tqdm(all_files, desc=\"Processing Hadith Files\"):\n",
        "#         all_hadith_data.extend(extract_hadith_data(file_path))\n",
        "\n",
        "#     # Save the merged data to a new JSON file\n",
        "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
        "#         json.dump(all_hadith_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# # Define the directories for Bukhari and Muslim hadith data\n",
        "# bukhari_directory = '/content/HadithsDorr/DATA/DistilitionOutput/Sahih_bukhari/'\n",
        "# muslim_directory = '/content/HadithsDorr/DATA/DistilitionOutput/Sahih_muslim/'\n",
        "\n",
        "# # Define the output file path\n",
        "# output_json_file = '/content/drive/MyDrive/merged_hadith_data.json'\n",
        "\n",
        "# # Merge the hadith files with a progress bar\n",
        "# merge_hadith_files(bukhari_directory, muslim_directory, output_json_file)\n",
        "\n",
        "# print(f\"Merged hadith data saved to {output_json_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# بيانات النظام (system message)\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are an AI assistant specialized in understanding and explaining Islamic Hadith.\",\n",
        "    \"You will be given a question related to a Hadith in Arabic.\",\n",
        "    \"Your task is to answer the question clearly, concisely, and accurately based on Islamic scholarship.\",\n",
        "    \"Respond in Arabic and follow the same tone and depth as classical Islamic explanations.\",\n",
        "    \"If the question is not related to Islamic Hadiths, respond with 'لا أستطيع المساعدة في هذا الموضوع'.\",\n",
        "    \"Don't provide any additional information or context that is not directly related to the Hadith.\",\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_finetunning_data = []\n",
        "\n",
        "# المسار الخاص بملف البيانات\n",
        "input_path = \"./formatted_hadiths.json\"\n",
        "# تأكد من أن الملف موجود\n",
        "if not os.path.exists(input_path):\n",
        "    raise FileNotFoundError(f\"File not found: {input_path}\")\n",
        "\n",
        "# قراءة البيانات وتحويلها\n",
        "with open(input_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)  # Load the entire data as a list of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "cODZv5QCqR7f"
      },
      "outputs": [],
      "source": [
        "for rec in data:\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# question:\",\n",
        "            rec[\"question\"]\n",
        "        ]),\n",
        "        \"input\": \"\",  \n",
        "        \"output\": \"/n\".join([\n",
        "            rec[\"answer\"]\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data saved to ./llm_finetunning_data.jsonl\n"
          ]
        }
      ],
      "source": [
        "# خلط البيانات\n",
        "random.Random(101).shuffle(llm_finetunning_data)\n",
        "\n",
        "# حفظ البيانات في ملف\n",
        "output_path = \"./llm_finetunning_data.jsonl\"\n",
        "with open(output_path, 'w', encoding='utf-8') as out_f:\n",
        "    for entry in llm_finetunning_data:\n",
        "        out_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Data saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cNUmjFzoqSAG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Train and evaluation datasets saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "# تحديد حجم التدريب\n",
        "train_sample_sz = int(len(llm_finetunning_data) * 0.8)  # 80% للتدريب\n",
        "\n",
        "# تقسيم البيانات إلى تدريب واختبار\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "# مسار الدليل الخاص بالبيانات\n",
        "data_dir = \"./\"  # يمكنك تعديل المسار هنا حسب احتياجك\n",
        "\n",
        "# إنشاء المجلدات إذا لم تكن موجودة\n",
        "os.makedirs(join(data_dir, \"dataset\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "# حفظ بيانات التدريب\n",
        "with open(join(data_dir, \"dataset\", \"llamafactory-finetune-data\", \"train.jsonl\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "# حفظ بيانات الاختبار\n",
        "with open(join(data_dir, \"dataset\", \"llamafactory-finetune-data\", \"val.jsonl\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "print(f\"✅ Train and evaluation datasets saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
