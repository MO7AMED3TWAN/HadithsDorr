{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c64ff",
   "metadata": {},
   "source": [
    "# Fitlers The Hadiths into 2 books only Sahih Bukhari and Muslim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e809280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# تحديد مسار ملف الأحاديث\n",
    "input_file = 'all_hadiths_explanations2.json'\n",
    "\n",
    "# التأكد من وجود الملف\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"الملف {input_file} غير موجود\")\n",
    "else:\n",
    "    # قراءة الملف\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        all_hadiths = json.load(f)\n",
    "    \n",
    "    print(f\"تم قراءة {len(all_hadiths)} حديث من الملف\")\n",
    "    \n",
    "    # الكتب المطلوبة للفلترة\n",
    "    target_books = [\"صحيح البخاري\", \"صحيح مسلم\"]\n",
    "    \n",
    "    # فلترة الأحاديث بناءً على قيمة book في data\n",
    "    filtered_hadiths = []\n",
    "    for hadith_item in all_hadiths:\n",
    "        # التحقق من أن الحديث يحتوي على explanation.data.book\n",
    "        if \"explanation\" in hadith_item and \\\n",
    "           \"data\" in hadith_item[\"explanation\"] and \\\n",
    "           \"book\" in hadith_item[\"explanation\"][\"data\"] and \\\n",
    "           hadith_item[\"explanation\"][\"data\"][\"book\"] in target_books:\n",
    "            filtered_hadiths.append(hadith_item)\n",
    "    \n",
    "    print(f\"عدد الأحاديث الكلي: {len(all_hadiths)}\")\n",
    "    print(f\"عدد الأحاديث بعد الفلترة: {len(filtered_hadiths)}\")\n",
    "    \n",
    "    # توزيع الأحاديث حسب الكتاب\n",
    "    book_counts = {}\n",
    "    for book in target_books:\n",
    "        count = sum(1 for hadith in filtered_hadiths if hadith[\"explanation\"][\"data\"][\"book\"] == book)\n",
    "        book_counts[book] = count\n",
    "    \n",
    "    for book, count in book_counts.items():\n",
    "        print(f\"عدد الأحاديث في {book}: {count}\")\n",
    "    \n",
    "    # حفظ الأحاديث المفلترة في ملف جديد\n",
    "    output_file = 'sahihain_explanationsV2.json'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_hadiths, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"تم حفظ {len(filtered_hadiths)} حديث في ملف {output_file}\")\n",
    "    \n",
    "    # عرض عينة من الأحاديث المفلترة\n",
    "    sample_size = min(5, len(filtered_hadiths))\n",
    "    sample_hadiths = filtered_hadiths[:sample_size]\n",
    "    \n",
    "    print(f\"عينة من {sample_size} أحاديث من صحيح البخاري وصحيح مسلم:\")\n",
    "    for i, hadith in enumerate(sample_hadiths, 1):\n",
    "        hadith_data = hadith[\"explanation\"][\"data\"]\n",
    "        print(f\"\\n{i}. كتاب: {hadith_data.get('book')}\")\n",
    "        print(f\"   راوي: {hadith_data.get('rawi')}\")\n",
    "        print(f\"   الحديث: {hadith_data.get('hadith', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0670",
   "metadata": {},
   "source": [
    "# Deleting unnecessary Keys in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa8f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def transform_hadith_data(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    تحويل بيانات الحديث من الشكل المعقد إلى الشكل المبسط\n",
    "    \n",
    "    Args:\n",
    "        input_file: مسار ملف الإدخال\n",
    "        output_file: مسار ملف الإخراج (اختياري)\n",
    "    \n",
    "    Returns:\n",
    "        list: قائمة بالأحاديث المعالجة\n",
    "    \"\"\"\n",
    "    # قراءة الملف\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"خطأ في قراءة الملف: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # قائمة لتخزين الأحاديث المعالجة\n",
    "    processed_hadiths = []\n",
    "    \n",
    "    # تحديد إذا كان الملف يحتوي على قائمة أو حديث واحد\n",
    "    if isinstance(data, list):\n",
    "        items = data\n",
    "    elif isinstance(data, dict) and \"hadith_id\" in data:\n",
    "        items = [data]\n",
    "    else:\n",
    "        print(\"صيغة الملف غير متوافقة\")\n",
    "        return []\n",
    "    \n",
    "    # معالجة كل عنصر\n",
    "    for item in items:\n",
    "        # إنشاء كائن الحديث المبسط\n",
    "        simplified_hadith = {}\n",
    "        \n",
    "        # إضافة معرف الحديث\n",
    "        if \"hadith_id\" in item:\n",
    "            simplified_hadith[\"hadith_id\"] = item[\"hadith_id\"]\n",
    "        \n",
    "        # استخراج البيانات من الهيكل المعقد\n",
    "        explanation_data = None\n",
    "        if \"explanation\" in item and \"data\" in item[\"explanation\"]:\n",
    "            explanation_data = item[\"explanation\"][\"data\"]\n",
    "        \n",
    "        # استخراج البيانات الأساسية\n",
    "        if explanation_data:\n",
    "            basic_fields = [\"hadith\", \"rawi\", \"mohdith\", \"book\", \"numberOrPage\", \"grade\", \"takhrij\"]\n",
    "            for field in basic_fields:\n",
    "                if field in explanation_data:\n",
    "                    simplified_hadith[field] = explanation_data[field]\n",
    "            \n",
    "            # استخراج الشرح\n",
    "            if \"hasSharhMetadata\" in explanation_data and explanation_data[\"hasSharhMetadata\"] and \"sharhMetadata\" in explanation_data:\n",
    "                sharh_metadata = explanation_data[\"sharhMetadata\"]\n",
    "                if \"sharh\" in sharh_metadata:\n",
    "                    simplified_hadith[\"sharh\"] = sharh_metadata[\"sharh\"]\n",
    "        \n",
    "        # إضافة الحديث المعالج إلى القائمة\n",
    "        if simplified_hadith:\n",
    "            processed_hadiths.append(simplified_hadith)\n",
    "    \n",
    "    # حفظ النتيجة إذا تم تحديد ملف الإخراج\n",
    "    if output_file and processed_hadiths:\n",
    "        try:\n",
    "            # إذا كان هناك حديث واحد فقط، نحفظه كعنصر واحد وليس قائمة\n",
    "            if len(processed_hadiths) == 1:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(processed_hadiths[0], f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(processed_hadiths, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"تم حفظ {len(processed_hadiths)} حديث في الملف: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"خطأ في حفظ الملف: {e}\")\n",
    "    \n",
    "    return processed_hadiths\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    معالجة كل ملفات JSON في مجلد معين\n",
    "    \n",
    "    Args:\n",
    "        input_dir: مسار مجلد الإدخال\n",
    "        output_dir: مسار مجلد الإخراج\n",
    "    \"\"\"\n",
    "    # التأكد من وجود مجلد الإخراج\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # عدد الملفات التي تمت معالجتها\n",
    "    processed_count = 0\n",
    "    \n",
    "    # معالجة كل ملف JSON في المجلد\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, f\"processed_{filename}\")\n",
    "            \n",
    "            # معالجة الملف\n",
    "            hadiths = transform_hadith_data(input_path, output_path)\n",
    "            \n",
    "            if hadiths:\n",
    "                processed_count += 1\n",
    "    \n",
    "    print(f\"تمت معالجة {processed_count} ملف من مجلد {input_dir}\")\n",
    "\n",
    "# مثال للاستخدام\n",
    "if __name__ == \"__main__\":\n",
    "    # معالجة ملف واحد\n",
    "    input_file = \"sahihain_explanationsV2.json\"\n",
    "    output_file = \"processed_all_hadithsV2.json\"\n",
    "    \n",
    "    transform_hadith_data(input_file, output_file)\n",
    "    \n",
    "    # معالجة مجلد كامل\n",
    "    # process_directory(\"input_folder\", \"output_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ca9b0",
   "metadata": {},
   "source": [
    "# Changing the Hadith_id to Numberorpage ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# تحميل البيانات\n",
    "with open('processed_El_Sahihain.json', 'r', encoding='utf-8') as f:\n",
    "    processed_El_Sahihain = json.load(f)\n",
    "\n",
    "# التأكد من وجود hadith_id في كل حديث\n",
    "for hadith in processed_El_Sahihain:\n",
    "    hadith_id = hadith.get('numberOrPage')\n",
    "    if hadith_id:\n",
    "        hadith['hadith_id'] = hadith_id\n",
    "\n",
    "# الترتيب التصاعدي حسب hadith_id (مع تحويله إلى رقم إن أمكن)\n",
    "processed_El_Sahihain_sorted = sorted(\n",
    "    processed_El_Sahihain,\n",
    "    key=lambda h: int(h.get('hadith_id', 0)) if str(h.get('hadith_id', '')).isdigit() else float('inf')\n",
    ")\n",
    "\n",
    "# حفظ الملف المرتب\n",
    "with open('sahih_bukhari_updated.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(processed_El_Sahihain_sorted, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20f5dd",
   "metadata": {},
   "source": [
    "# Cleaning the Sharh using the repeated Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithCleaner:\n",
    "    def __init__(self, explanation_markers, isnad_markers):\n",
    "        self.explanation_markers = explanation_markers\n",
    "        self.isnad_markers = isnad_markers\n",
    "\n",
    "    def extract_explanation(self, sharh):\n",
    "        # Step 1: Remove the hadith text if it appears at the beginning\n",
    "        hadith_text_end = sharh.find('\\n     الراوي :')\n",
    "        if hadith_text_end != -1:\n",
    "            sharh = sharh[hadith_text_end:]\n",
    "        \n",
    "        # Step 2: Remove metadata section\n",
    "        metadata_pattern = re.compile(r'\\s*الراوي :.*?التخريج :.*?\\n\\s*\\n', re.DOTALL)\n",
    "        match = metadata_pattern.search(sharh)\n",
    "        if match:\n",
    "            after_metadata = sharh[match.end():]\n",
    "        else:\n",
    "            isnad_regex = r'(.*?)\\bالتخريج\\s*:.*?\\n+'\n",
    "            match = re.search(isnad_regex, sharh, re.DOTALL)\n",
    "            after_metadata = sharh[match.end():] if match else sharh\n",
    "\n",
    "        # Step 3: Process text parts\n",
    "        parts = [p.strip() for p in after_metadata.split('\\n\\n') if p.strip()]\n",
    "        if parts and not any(marker in parts[0] for marker in self.isnad_markers):\n",
    "            return parts[0]\n",
    "        \n",
    "        # Step 4: Look for explanation markers\n",
    "        for marker in self.explanation_markers:\n",
    "            idx = after_metadata.find(marker)\n",
    "            if idx != -1:\n",
    "                return after_metadata[idx:].strip()\n",
    "        \n",
    "        # Step 5: Find substantial paragraph\n",
    "        for para in parts:\n",
    "            if len(para) > 50 and not any(marker in para for marker in self.isnad_markers):\n",
    "                return para\n",
    "        \n",
    "        return after_metadata.strip()\n",
    "\n",
    "    def process_hadith(self, hadith):\n",
    "        cleaned = dict(hadith)\n",
    "        if 'sharh' in hadith and hadith['sharh']:\n",
    "            cleaned['sharh'] = self.extract_explanation(hadith['sharh'])\n",
    "        return cleaned\n",
    "\n",
    "    def clean_hadiths(self, input_path, output_path):\n",
    "        \"\"\"\n",
    "        Clean hadiths from input file and save to output file.\n",
    "        \n",
    "        Args:\n",
    "            input_path (str): Path to input JSON file\n",
    "            output_path (str): Path to save cleaned output JSON file\n",
    "        \n",
    "        Returns:\n",
    "            list: Cleaned hadiths\n",
    "        \"\"\"\n",
    "        print(f\"Loading hadiths from {input_path}...\")\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            hadiths = json.load(f)\n",
    "        print(f\"Loaded {len(hadiths)} hadiths.\")\n",
    "\n",
    "        cleaned_hadiths = []\n",
    "        print(\"Processing hadiths...\")\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = list(tqdm(\n",
    "                executor.map(self.process_hadith, hadiths),\n",
    "                total=len(hadiths),\n",
    "                desc=\"Cleaning hadiths\",\n",
    "                unit=\"hadith\"\n",
    "            ))\n",
    "            cleaned_hadiths.extend(futures)\n",
    "\n",
    "        print(f\"Saving cleaned hadiths to {output_path}...\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_hadiths, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✓ Successfully processed and saved {len(cleaned_hadiths)} hadiths.\")\n",
    "        return cleaned_hadiths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410de6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded list of explanation markers based on analysis\n",
    "EXPLANATION_MARKERS = [\n",
    "    'هذا الحديثُ', 'مُناسبةُ هذا الحديثِ', 'وفي هذا الحديثِ', 'وفي هذا الأثرِ', 'وفي الحديثِ',\n",
    "    'وفيه:', 'وفي هذا يَقولُ', 'وفي هذا الأثَرِ', 'وفي هذا الحَديثِ', 'وفي هذا يَحكي',\n",
    "    'وفي هذا يَذكُرُ', 'وفي هذا يَروي', 'وفي هذا يَخبِرُ', 'وفي هذا يَقولُ',\n",
    "    'في هذا الحديثِ', 'يُبيِّن', 'يَدُلُّ', 'أخبر', 'بيَّن', 'يُخبر', 'يُبين', 'يَروي', \n",
    "    'تَروي', 'يَذكر', 'تَذكر', 'يَحكي', 'تَحكي'\n",
    "]\n",
    "\n",
    "ISNAD_MARKERS = [\n",
    "    'الراوي', 'المحدث', 'المصدر', 'الصفحة أو الرقم', 'خلاصة حكم المحدث', 'التخريج'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7987caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HadithCleaner= HadithCleaner(\n",
    "    explanation_markers=EXPLANATION_MARKERS,\n",
    "    isnad_markers=ISNAD_MARKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hadiths from sahih_muslim.json...\n",
      "Loaded 4288 hadiths.\n",
      "Processing hadiths...\n"
     ]
    }
   ],
   "source": [
    "HadithCleaner.clean_hadiths(\"sahih_muslim.json\", \"sahih_muslim_cleaned2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "HadithCleaner.clean_hadiths(\"sahih_bukhari.json\", \"sahih_bukhari_cleaned2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85f9e5",
   "metadata": {},
   "source": [
    "# Spliting each Headith to josn files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63970f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadithSplitter:\n",
    "    def __init__(self, input_file, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the HadithSplitter with input and output paths.\n",
    "        \n",
    "        Args:\n",
    "            input_file (str): Path to input JSON file containing hadiths\n",
    "            output_dir (str): Directory path where individual hadith files will be saved\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def sanitize_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Sanitize filename by removing invalid characters.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Original filename\n",
    "            \n",
    "        Returns:\n",
    "            str: Sanitized filename\n",
    "        \"\"\"\n",
    "        sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', str(filename))\n",
    "        sanitized = sanitized.replace(' ', '_')\n",
    "        return sanitized\n",
    "\n",
    "    def create_output_directory(self):\n",
    "        \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            print(f\"Created directory: {self.output_dir}\")\n",
    "\n",
    "    def load_hadiths(self):\n",
    "        \"\"\"\n",
    "        Load hadiths from input JSON file.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of hadith dictionaries\n",
    "        \"\"\"\n",
    "        print(\"Loading hadiths from file...\")\n",
    "        with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "            hadiths = json.load(f)\n",
    "        print(f\"Found {len(hadiths)} hadiths to process.\")\n",
    "        return hadiths\n",
    "\n",
    "    def split_hadiths(self):\n",
    "        \"\"\"Split hadiths into individual files.\"\"\"\n",
    "        self.create_output_directory()\n",
    "        hadiths = self.load_hadiths()\n",
    "\n",
    "        for hadith in tqdm(hadiths, desc=\"Splitting hadiths\", unit=\"hadith\"):\n",
    "            hadith_id = hadith.get('hadith_id', 'unknown')\n",
    "            safe_filename = self.sanitize_filename(hadith_id)\n",
    "            output_file = os.path.join(self.output_dir, f\"{safe_filename}.json\")\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(hadith, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"✓ Successfully split {len(hadiths)} hadiths into individual files in {self.output_dir}\")\n",
    "        \n",
    "        # Display sample of created files\n",
    "        print(\"\\nSample of files created:\")\n",
    "        for filename in sorted(os.listdir(self.output_dir))[:5]:\n",
    "            print(f\"  - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # For Bukhari\n",
    "    bukhari_splitter = HadithSplitter(\n",
    "        input_file='bukhari_cleaned_hadiths.json',\n",
    "        output_dir='d:\\\\hadith after Q&As\\\\Sahih_bukhari'\n",
    "    )\n",
    "    bukhari_splitter.split_hadiths()\n",
    "    \n",
    "    # For Muslim\n",
    "    muslim_splitter = HadithSplitter(\n",
    "        input_file='sahih_muslim_cleaned_hadiths.json',\n",
    "        output_dir='d:\\\\hadith after Q&As\\\\Sahih_muslim'\n",
    "    )\n",
    "    muslim_splitter.split_hadiths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963db5eb",
   "metadata": {},
   "source": [
    "# Display all original and cleaned sharh for comparison and save to JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = []\n",
    "for idx, (original, cleaned) in enumerate(zip(hadiths_BU, clean_hadiths_BU)):\n",
    "    original_sharh = original.get('sharh', '')\n",
    "    cleaned_sharh = cleaned.get('sharh', '')\n",
    "    comparison.append({\n",
    "        \"hadith_index\": idx,\n",
    "        \"hadith_id\": original.get('hadith_id', ''),\n",
    "        \"original_sharh\": original_sharh,\n",
    "        \"cleaned_sharh\": cleaned_sharh,\n",
    "        \"sharh_match\": original_sharh == cleaned_sharh\n",
    "    })\n",
    "    print(f\"Hadith Index: {idx}\")\n",
    "    print(\"Original Sharh:\")\n",
    "    print(original_sharh)\n",
    "    print(\"\\nCleaned Sharh:\")\n",
    "    print(cleaned_sharh)\n",
    "    print(\"\\nSharh Match:\", original_sharh == cleaned_sharh)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save comparison to JSON\n",
    "with open('Bukhari_sharh_comparison.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(comparison, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
